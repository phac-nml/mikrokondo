{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#welcome-to-mikrokondo","title":"Welcome to mikrokondo!","text":""},{"location":"#what-is-mikrokondo","title":"What is mikrokondo?","text":"<p>Mikrokondo is a tidy workflow for performing routine bioinformatic assessment of sequencing reads and assemblies, such as: read pre-processing, assessing contamination, assembly, quality assessment of assemblies, and pathogen-specific typing. It is easily configurable, provides dynamic dispatch of species specific workflows and produces common outputs.</p>"},{"location":"#what-is-the-target-audience","title":"What is the target audience?","text":"<p>This workflow can be used in sequencing and reference laboratories as a part of an automated quality and initial bioinformatics assessment protocol.</p>"},{"location":"#is-mikrokondo-right-for-me","title":"Is mikrokondo right for me?","text":"<p>Mikrokondo is purpose built to provide sequencing and clinical laboratories with an all encompassing workflow to provide a standardized workflow that can provide the initial quality assessment of sequencing reads and assemblies, and initial pathogen-specific typing. It has been designed to be configurable so that new tools and quality metrics can be easily incorporated into the workflow to allow for automation of these routine tasks regardless of pathogen of interest. It currently accepts Illumina, Nanopore or Pacbio (Pacbio data only partially tested) sequencing data. It is capable of hybrid assembly or accepting pre-assembled genomes.</p> <p>This workflow will detect what pathogen(s) is present and apply the applicable metrics and genotypic typing where appropriate, generating easy to read and understand reports. If your group is regularly sequencing or analyzing genomic sequences, implementation of this workflow will automate the hands-on time time usually required for these common bioinformatic tasks.</p>"},{"location":"#whole-genome-and-metagenomic-samples-typical-workflow-differences","title":"Whole genome and Metagenomic samples typical workflow differences","text":"<p>This pipeline has been written to automatically detect if a sample contains more than one organism. Whether it is intentional (as in shotgun metagenomics) or contamination, a sample with more than one organism detected via Mash will be marked as <code>metagenomic</code>. </p> <p>Typical workflow for whole genome sample (one organism):</p> <ol> <li>Reads are cleaned of human DNA, trimmed and one organism is detected</li> <li>Reads are assembled into contigs, then polished</li> <li>Assemblies are run through quality tools</li> <li>Assemblies are speciated with appropriate tool</li> <li>Species specific subtyping tools are called based on speciation from step 4.</li> </ol> <p>Typical workflow for metagenomic sample (greater than one organism):</p> <ol> <li>Reads are cleaned of human DNA, trimmed and multiple organisms are detected</li> <li>Reads are assembled into contigs, then polished</li> <li>Assemblies undergo contig binning (via Kraken2)</li> <li>Each bin is run through quality tools</li> <li>Each bin is speciated with appropriate tool</li> <li>Species specific subtyping tools are called on each bin based on speciation from step 5.</li> </ol>"},{"location":"#workflow-schematics-subject-to-change","title":"Workflow Schematics (Subject to change)","text":""},{"location":"subworkflows/assemble_reads/","title":"Assembly","text":""},{"location":"subworkflows/assemble_reads/#subworkflowslocalassemble_reads","title":"subworkflows/local/assemble_reads","text":"<p>NOTE: Hybrid assembly of long and short reads uses a different workflow that can be found here</p>"},{"location":"subworkflows/assemble_reads/#steps","title":"Steps","text":"<ol> <li>Assembly proceeds differently depending whether paired-end short or long reads. If the samples are marked as metagenomic, then metagenomic assembly flags will be added to the corresponding assembler.</li> <li>Paired end assembly is performed using Spades (for more information see the module spades_assemble.nf)</li> <li> <p>Long read assembly is performed using Flye (for more information see the module flye_assemble.nf</p> </li> <li> <p>Bandage plots are generated using Bandage, these images were included as they can be informative of assembly quality in some situations bandage_image.nf.</p> </li> <li> <p>Polishing (OPTIONAL) can be performed on either short or long/hybrid assemblies. Minimap2 is used to create a contig index minimap2_index.nf and then maps reads to that index minimap2_map.nf. Lastly, Racon uses this output to perform contig polishing racon_polish.nf. To turn off polishing add the following to your command line parameters <code>--skip_polishing</code>.</p> </li> </ol>"},{"location":"subworkflows/assemble_reads/#input","title":"Input","text":"<ul> <li>Processed Reads from clean reads subworkflow</li> </ul>"},{"location":"subworkflows/assemble_reads/#output","title":"Output","text":"<ul> <li>Reads</li> <li>FinalReads<ul> <li>SAMPLE</li> </ul> </li> <li>Processing<ul> <li>Dehosting</li> <li>Trimmed<ul> <li>FastP</li> <li>MashSketches</li> </ul> </li> </ul> </li> </ul>"},{"location":"subworkflows/bin_contigs/","title":"Bin Contigs","text":""},{"location":"subworkflows/bin_contigs/#subworkflowslocalsplit_metagenomicnf","title":"subworkflows/local/split_metagenomic.nf","text":""},{"location":"subworkflows/bin_contigs/#steps","title":"Steps","text":"<ol> <li>Kraken2 is run to generate output reports and separate classified contigs from unclassified.</li> <li>A custom script separates each classified group of contigs into separate files at a specified taxonomic level (default level: genus). Output files are labeled as <code>[Sample Name]_[Genus]</code> to allow for easy post processing.</li> </ol>"},{"location":"subworkflows/bin_contigs/#input","title":"Input","text":"<ul> <li>Contig file (fasta) from the <code>FinalAssembly</code> dir<ul> <li>This is the final contig file from the last step in the CleanAssemble workflow (taking into account any skip flags that have been used)</li> </ul> </li> <li>metadata from prior tools</li> </ul>"},{"location":"subworkflows/bin_contigs/#outputs","title":"Outputs","text":"<ul> <li>Assembly<ul> <li>PostProcessing<ul> <li>Metagenomic<ul> <li>BinnedContigs<ul> <li>SAMPLE<ul> <li>CONTIG</li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"subworkflows/clean_reads/","title":"Read Quality Control","text":""},{"location":"subworkflows/clean_reads/#subworkflowslocalclean_reads","title":"subworkflows/local/clean_reads","text":""},{"location":"subworkflows/clean_reads/#steps","title":"Steps","text":"<ol> <li> <p>Reads are decontaminated using minimap2, against a 'sequencing off-target' index. This index contains:</p> <ul> <li>Reads associated with Humans (de-hosting)</li> <li>Known sequencing controls (phiX)</li> </ul> </li> <li> <p>Read quality filtering and trimming is performed using FastP</p> <ul> <li>Currently no adapters are specified within FastP when it is run and auto-detection is used.</li> <li>FastP parameters can be altered within the nextflow.config file.</li> <li>Long read data is also run through FastP for gathering of summary data, however long read (un-paired reads) trimming is not performed and only summary metrics are generated. Chopper is currently integrated in MikroKondo but it has been removed from this workflow due to a lack of interest in quality trimming of long read data. It may be reintroduced in the future upon request.</li> </ul> </li> <li> <p>Genome size estimation is performed using Mash Sketch of reads and estimated genome size is output.</p> </li> <li> <p>Read down sampling (OPTIONAL) an estimated depth threshold can be specified to down sample large read sets. This step can be used to improve genome assembly quality, and is something that can be found in other assembly pipelines such as Shovill. To disable down sampling add <code>--skip_depth_sampling true</code> to your command line.</p> <ul> <li>Depth is estimated by using the estimated genome size output from Mash</li> <li>Total base pairs are taken from FastP</li> <li>Read down sampling is then performed using Seqtk (Illumina) or Rasusa (Nanopore or Pacbio).</li> </ul> </li> <li> <p>Metagenomic assessment using a custom Mash 'sketch' file generated from the Genome Taxonomy Database GTDB and the mash_screen module, this step assesses how many bacterial genera are present in a sample (e.g. a contaminated or metagenomic sample may have more than one genus of bacteria present) with greater than 90% identity (according to Mash). When more than 1 taxa are present, the metagenomic tag is set, turning on metagenomic assembly in later steps. Additionally Kraken2 will be run on metagenomic assemblies and contigs will be binned at a defined taxonomic level (default level: genus).</p> </li> <li> <p>Nanopore ID screening duplicate Nanopore read ID's have been known to cause issues in the pipeline downstream. In order to bypass this issue, an option can be toggled where a script will read in Nanopore reads and append a unique ID to the header, this process can be slow so default setting is to skip, <code>--skip_ont_header_cleaning true</code>.</p> </li> </ol>"},{"location":"subworkflows/clean_reads/#input","title":"Input","text":"<ul> <li>Next generation sequencing reads:<ul> <li>Short read - Illumina</li> <li>Long read:<ul> <li>Nanopore</li> <li>Pacbio</li> </ul> </li> </ul> </li> <li>User submitted sample sheet</li> </ul>"},{"location":"subworkflows/clean_reads/#outputs","title":"Outputs","text":"<ul> <li>Reads<ul> <li>FinalReads<ul> <li>SAMPLE</li> </ul> </li> <li>Processing<ul> <li>Dehosting<ul> <li>Trimmed<ul> <li>FastP<ul> <li>Seqtk</li> </ul> </li> <li>MashSketches</li> </ul> </li> </ul> </li> </ul> </li> <li>Quality<ul> <li>RawReadQuality</li> <li>Trimmed<ul> <li>FastP</li> <li>MashScreen</li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"subworkflows/determine_species/","title":"Determine Species","text":""},{"location":"subworkflows/determine_species/#subworkflowslocaldetermine_species","title":"subworkflows/local/determine_species","text":""},{"location":"subworkflows/determine_species/#steps","title":"Steps","text":"<ol> <li>Taxonomic classification is completed using Mash (DEFAULT; mash_screen.nf), or Kraken2 (OPTIONAL, or when samples are flagged metagenomic; kraken.nf). Species classification and subsequent subtyping can be skipped by passing <code>--skip_species_classification true</code> on the command line. To select Kraken2 for speciation rather than mash add <code>--run_kraken true</code> to your command line arguments.</li> </ol>"},{"location":"subworkflows/determine_species/#input","title":"Input","text":"<ul> <li>Contig file (fasta) from the <code>FinalAssembly</code> dir<ul> <li>This is the final contig file from the last step in the CleanAssemble workflow (taking into account any skip flags that have been used)</li> </ul> </li> </ul>"},{"location":"subworkflows/determine_species/#output","title":"Output","text":"<ul> <li>Assembly<ul> <li>PostProcessing<ul> <li>Speciation<ul> <li>MashScreen</li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"subworkflows/genomes_annotate/","title":"Genome Annotation","text":""},{"location":"subworkflows/genomes_annotate/#subworkflowslocalannotate_genomes","title":"subworkflows/local/annotate_genomes","text":""},{"location":"subworkflows/genomes_annotate/#steps","title":"Steps","text":"<ol> <li> <p>Genome annotation is performed using Bakta within bakta_annotate.nf</p> <ul> <li>You must download a Bakta database and add its path to the nextflow.config file or add its path as a command line option</li> <li>To skip running Bakta add <code>--skip_bakta true</code> to your command line options.</li> </ul> </li> <li> <p>Screening for antimicrobial resistance Abricate is used with the default options and default database, however you can specify a database by updating the <code>args</code> in the nextflow.config for Abricate.</p> <ul> <li>You can skip running Abricate by adding <code>--skip_abricate true</code> to your command line options.</li> </ul> </li> <li> <p>Screening for plasmids is performed using Mob-suite with default options.</p> </li> <li> <p>Selection of Pointfindr database. This step is only ran if running StarAMR. It will try and select the correct database based on the species identified earlier in the pipeline. If a database cannot be determined pointfinder will simply not be run.</p> </li> <li> <p>Exporting of StarAMR databases used. To provide a method of user validation for automatic database selection, the database info from StarAMR will be exported from the pipeline into the file <code>StarAMRDBVersions.txt</code> and placed in the StarAMR directory.</p> </li> <li> <p>Screening for antimicrobial resistance with StarAMR. StarAMR is provided as an additional option to screen for antimicrobial resistance in ResFinder, PointFinder and PlasmidFinder databases. Passing in a database is optional as the one within the container will be used by default.</p> <ul> <li>You can skip running StarAMR by adding the following flag <code>--skip_starmar</code></li> </ul> </li> </ol> <p>NOTE: A custom database for Bakta can be downloaded via the commandline using <code>bakta_download_db.nf</code>. The <code>bakta_db</code> setting can be changed in the <code>nextflow.config</code> file, see bakta</p>"},{"location":"subworkflows/genomes_annotate/#input","title":"Input","text":"<ul> <li>Contig file (fasta) from the <code>FinalAssembly</code> dir<ul> <li>This is the final contig file from the last step in the CleanAssemble workflow (taking into account any skip flags that have been used)</li> </ul> </li> <li>metadata from prior tools</li> </ul>"},{"location":"subworkflows/genomes_annotate/#output","title":"Output","text":"<ul> <li>Assembly<ul> <li>Annotation<ul> <li>Abricate</li> <li>Mobsuite<ul> <li>recon<ul> <li>SAMPLE</li> </ul> </li> </ul> </li> <li>StarAMR<ul> <li>SAMPLE</li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"subworkflows/hybrid_assembly/","title":"Hybrid Assembly","text":""},{"location":"subworkflows/hybrid_assembly/#subworkflowslocalhybrid_assembly","title":"subworkflows/local/hybrid_assembly","text":""},{"location":"subworkflows/hybrid_assembly/#choice-of-2-workflows","title":"Choice of 2 workflows","text":"<ol> <li>DEFAULT     A. Flye assembly flye_assembly.nf     B. Bandage creates a bandage plot of the assembly bandage_image.nf     C. Minimap2 creates an index of the contigs (minimap2_index.nf), then maps long reads to this index minimap2_map.nf     D. Racon uses the short reads to iteratively polish contigs pilon_iter.nf</li> <li>OPTIONAL     A. Unicycler assembly unicycler_assemble.nf     B. Bandage creates a bandage plot of the assembly bandage_image.nf</li> </ol>"},{"location":"subworkflows/hybrid_assembly/#input","title":"Input","text":"<ul> <li>Next generation sequencing reads:<ul> <li>Short read<ul> <li>Illumina</li> </ul> </li> <li>Long read:<ul> <li>Nanopore</li> <li>Pacbio</li> </ul> </li> </ul> </li> <li>User submitted sample sheet   </li> </ul>"},{"location":"subworkflows/hybrid_assembly/#output","title":"Output","text":"<ul> <li>Assembly<ul> <li>Assembling<ul> <li>Bandage</li> <li>ConsensusGeneration<ul> <li>Polishing<ul> <li>Pilon<ul> <li>BAMs</li> <li>Changes</li> <li>Fasta</li> <li>VCF</li> </ul> </li> </ul> </li> <li>Racon<ul> <li>Consensus</li> </ul> </li> </ul> </li> <li>Unicycler</li> </ul> </li> <li>FinalAssembly<ul> <li>SAMPLE</li> </ul> </li> </ul> </li> </ul>"},{"location":"subworkflows/input_check/","title":"Input Verification","text":""},{"location":"subworkflows/input_check/#subworkflowslocalinput_checknf","title":"subworkflows/local/input_check.nf","text":""},{"location":"subworkflows/input_check/#steps","title":"Steps","text":"<ol> <li> <p>Reads in the sample sheet and groups samples with shared IDs. </p> </li> <li> <p>Pipeline specific tags are added to each sample.</p> </li> <li> <p>If there are samples that have duplicate ID's the samples will be combined.</p> </li> </ol>"},{"location":"subworkflows/input_check/#input","title":"Input","text":"<ul> <li>User-submitted CSV formatted sample sheet</li> </ul>"},{"location":"subworkflows/input_check/#outputs","title":"Outputs","text":"<ul> <li>A channel of reads and their associated tags</li> </ul>"},{"location":"subworkflows/polish_assemblies/","title":"Assembly Polishing","text":""},{"location":"subworkflows/polish_assemblies/#subworkflowslocalpolish_assemblies","title":"subworkflows/local/polish_assemblies","text":""},{"location":"subworkflows/polish_assemblies/#steps","title":"Steps","text":"<ol> <li> <p>Final polishing proceeds differently depending on whether the sample is Illumina or Pacbio.</p> </li> <li> <p>Illumina A custom script is implemented to iteratively polish assemblies with reads based on a set number of iterations (DEFAULT 3). </p> <ul> <li>Polishing uses Pilon and minimap2, with reads being mapped back to the polished assembly each time.</li> </ul> </li> <li>Nanopore Medaka consensus is used to polish reads, a model must be specified by the user for polishing.</li> <li>Pacbio No addtional polishing is performed, outputs of Pacbio data still need to be tested.</li> </ol>"},{"location":"subworkflows/polish_assemblies/#input","title":"Input","text":"<ul> <li>cleaned reads (<code>fastq</code>) from the <code>FinalReads</code> dir</li> <li>This is the final reads file from the last step in the <code>Clean Reads</code> workflow (taking into account any skip flags that have been used)</li> <li>Contig file (<code>fasta</code>) from the <code>FinalAssembly</code> dir</li> <li>This is the final contig file from the last step in the CleanAssemble workflow (taking into account any skip flags that have been used)</li> </ul>"},{"location":"subworkflows/polish_assemblies/#outputs","title":"Outputs","text":"<ul> <li>Assembly</li> <li>Assembling<ul> <li>ConsensusGeneration<ul> <li>Polishing<ul> <li>Pilon<ul> <li>BAMs</li> <li>Changes</li> <li>Fasta</li> <li>VCF</li> </ul> </li> <li>Racon<ul> <li>Consensus</li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"subworkflows/qc_assembly/","title":"Assembly Quality Control","text":""},{"location":"subworkflows/qc_assembly/#subworkflowslocalqc_assembly","title":"subworkflows/local/qc_assembly","text":""},{"location":"subworkflows/qc_assembly/#steps","title":"Steps","text":"<ol> <li> <p>Generate assembly quality metrics QUAST is used to generate summary assembly metrics such as: N50 value, number of contigs,average depth of coverage and genome size.</p> </li> <li> <p>Assembly filtering Using a custom nexflow DSL (Groovy)script, assemblies are filtered to meet quality thresholds.</p> </li> <li> <p>See nextflow.config in the <code>quast_filter</code> section to see what defaults are currently implemented, or to set your own.</p> </li> <li> <p>Contamination detection CheckM is run to identify a percent contamination score and build up evidence for signs of contamination in a sample. </p> </li> <li> <p>CheckM can be skipped by adding <code>--skip_checkm</code> to the command-line options as the data it generates may not be needed, and it can have a long run time.</p> </li> <li> <p>Classic seven gene MLST mlst is run and its outputs are contained within the final report. </p> </li> <li> <p>This step can be skipped by adding <code>--skip_mlst</code> to the commmand line options.</p> </li> </ol>"},{"location":"subworkflows/qc_assembly/#input","title":"Input","text":"<ul> <li>cleaned reads (<code>fastq</code>) from the <code>FinalReads</code> dir</li> <li>This is the final reads file from the last step in the <code>Clean Reads</code> workflow (taking into account any skip flags that have been used)</li> <li>Contig file (<code>fasta</code>) from the <code>FinalAssembly</code> dir</li> <li>This is the final contig file from the last step in the CleanAssemble workflow (taking into account any skip flags that have been used)</li> </ul>"},{"location":"subworkflows/qc_assembly/#outputs","title":"Outputs","text":"<ul> <li>Assembly</li> <li>Quality<ul> <li>CheckM<ul> <li>SAMPLE<ul> <li>bins</li> <li>storage</li> <li>tree</li> </ul> </li> </ul> </li> <li>Quast<ul> <li>SAMPLE</li> </ul> </li> </ul> </li> <li>Subtyping</li> <li>SevenGeneMLST</li> <li>mlst</li> </ul>"},{"location":"subworkflows/subtype_genome/","title":"Genome Sub-typing","text":""},{"location":"subworkflows/subtype_genome/#subworkflowslocalsubtype_genome","title":"subworkflows/local/subtype_genome","text":""},{"location":"subworkflows/subtype_genome/#steps","title":"Steps","text":"<ol> <li> <p>Species specific subtyping tools are launched according to the pipeline's Mash screen report.</p> </li> <li> <p>Currently subtyping tools for Escherichia, Salmonella, Listeria spp., Staphylococcus spp., Klebsiella spp. and Shigella spp. are supported.</p> </li> <li>Subtyping can be disabled from the command line by passing <code>--skip_subtyping true</code> on the command line.</li> </ol> <p>NOTE If a sample cannot be subtyped, it merely passes through the pipeline and is not typed. A log message will instead be displayed notifying the user the sample cannot be typed.</p>"},{"location":"subworkflows/subtype_genome/#input","title":"Input","text":"<ul> <li>Contig file (fasta) from the <code>FinalAssembly</code> dir</li> <li>This is the final contig file from the last step in the <code>CleanAssemble</code> workflow (taking into account any skip flags that have been used)</li> <li>Mash report from assembly speciation step in the <code>CleanAssemble</code> workflow</li> </ul>"},{"location":"subworkflows/subtype_genome/#output","title":"Output","text":"<ul> <li>Subtyping</li> <li>ECTyper<ul> <li>SAMPLE</li> </ul> </li> <li>SevenGeneMLST</li> <li>SISTR</li> <li>Etc...</li> </ul>"},{"location":"troubleshooting/FAQ/","title":"FAQ","text":""},{"location":"troubleshooting/FAQ/#how-is-variable-type-determined-from-command-line-parameters","title":"How is variable type determined from command line parameters?","text":"<p>In a situation where you are developing the pipeline or finding that the parameter passed on the command line is not working as expected, for example, example: the user wants a sample to have at least 1000 reads before going for assembly (<code>--min_reads 1000</code>) and samples with less than 1000 reads are passing onto the assembly step.</p> <p>The way a variable type is determined from the command line can be found in the below groovy code. The snippet is also pasted below and is up to date as of 2023-10-16:</p> <pre><code>    static protected parseParamValue(String str ) {\n\n        if ( str == null ) return null\n\n        if ( str.toLowerCase() == 'true') return Boolean.TRUE\n        if ( str.toLowerCase() == 'false' ) return Boolean.FALSE\n\n        if ( str==~/\\d+(\\.\\d+)?/ &amp;&amp; str.isInteger() ) return str.toInteger()\n        if ( str==~/\\d+(\\.\\d+)?/ &amp;&amp; str.isLong() ) return str.toLong()\n        if ( str==~/\\d+(\\.\\d+)?/ &amp;&amp; str.isDouble() ) return str.toDouble()\n\n        return str\n    }\n</code></pre>"},{"location":"troubleshooting/FAQ/#troubleshooting","title":"Troubleshooting","text":""},{"location":"troubleshooting/FAQ/#common-errors-and-how-to-maybe-fix-them","title":"Common errors and how to (maybe) fix them","text":""},{"location":"troubleshooting/FAQ/#null-errors-or-report-generation-failing-on-line-701","title":"null errors, or report generation failing on line 701","text":"<p>Currently there are compatibility issues between version 22 and 23.10.0 of nextflow with regards to parsing the <code>nextflow.config</code> file. I am currently working on addressing them now. if you happen to encounter issues please downgrade your nextflow install to 22.10.1.</p>"},{"location":"troubleshooting/FAQ/#permission-denied-on-a-python-script-binsome_scriptpy","title":"Permission denied on a python script (<code>bin/some_script.py</code>)","text":"<p>On some installs, a lack of permissions for python scripts are causing this error to occur. The easiest way to solve this issue is to execute <code>chmod +x bin/*.py</code> in the mikrokondo installation directory. This will add execution permissions to all of the scripts, if this solution does not work then please submit an issue.</p>"},{"location":"troubleshooting/FAQ/#random-issues-containing-on-resume-orgiq80leveldbimplversionretain","title":"Random issues containing on resume <code>org.iq80.leveldb.impl.Version.retain()</code>","text":"<p>Sometimes the resume features of Nextflow don't work completely. The above error string typically implies that some output could not be gathered from a process and on subsequent resumes you will get an error. You can find out what process (and its work directory location) caused the error in the <code>nextflow.log</code> (normally it will be at the top of some long traceback in the log), and a work directory will be specified listing the directory causing the error. Delete this directory and resume the pipeline. If you hate logs and you don't care about resuming you can simply delete the work directory entirely.</p>"},{"location":"troubleshooting/FAQ/#staramr","title":"StarAMR","text":"<ul> <li>Exit code 1, and an error involving <code>stderr=FASTA-Reader: Ignoring invalid residues at position(s):</code></li> <li>This is likely not a problem with your data but with your databases, following the instructions listed here should fix the issue.</li> <li>The command to download the proper databases mentioned in the issue is listed here:   <code>staramr db build --dir staramr_databases --resfinder-commit fa32d9a3cf0c12ec70ca4e90c45c0d590ee810bd --pointfinder-commit 8c694b9f336153e6d618b897b3b4930961521eb8 --plasmidfinder-commit c18e08c17a5988d4f075fc1171636e47546a323d</code></li> <li>Passing in a database is optional as the one within the container will be used by default.</li> <li>If you continue to have problems with StarAMR you can skip it using <code>--skip_staramr</code></li> </ul>"},{"location":"troubleshooting/FAQ/#common-mash-errors","title":"Common mash errors","text":"<ul> <li>Mash exit code 139 or 255, you may see <code>org.iq80.leveldb.impl.Version.retain()</code> appearing on screen as well.</li> <li>This indicates a segmentation fault, due to mash failing or alternatively some resource not being available. If you see that mash has run properly in the work directory output but Nextflow is saying the process failed and the <code>versions.yml</code> file is missing you likely have encountered some resource limit on your system. A simple solution is likely to reduce the number of <code>maxForks</code> available to the different Mash processes in the <code>conf/modules.config</code> file. Alternatively you may need to alter the number in some Nextflow environment variables e.g. <code>OMP_NUM_THREADS</code>, <code>USE_SIMPLE_THREADED_LEVEL3</code> and <code>OPENBLAS_NUM_THREADS</code>.</li> </ul>"},{"location":"troubleshooting/FAQ/#common-spades-issues","title":"Common spades issues","text":"<ul> <li>Spades exit code 21</li> <li> <p>One potential cause of this issue (requires looking at the log files) is due to not enough reads being present. You can avoid samples with too few reads going to assembly by adjusting the <code>min_reads</code> parameter in the <code>nextflow.config</code>. It can also be adjusted from the command line with the flag <code>--min_reads 1000</code></p> </li> <li> <p>Spades exit code 245</p> </li> <li>This could be due to multiple issues and typically results from a segmentation fault (OS Code 11). Try increasing the amount of memory spades is alloted (base.config) if the issue persists try using a different Spades container/ create an issue.</li> </ul>"},{"location":"troubleshooting/FAQ/#common-kraken2-issues","title":"Common Kraken2 issues","text":"<ul> <li>Kraken2 exit code 2</li> <li>It is still a good idea to look at the output logs to verify your issue as they may say something like: <code>kraken2: database (\"./kraken2_database\") does not contain necessary file taxo.k2d</code> despite the taxo.k2d file being present. This is potentially caused by symlink issues, and one possible fix is to provide the absolute path to your Kraken2 database in the nextflow.config or from the command line <code>--kraken.db /PATH/TO/DB</code></li> </ul>"},{"location":"troubleshooting/FAQ/#common-docker-issues","title":"Common Docker issues","text":"<ul> <li>Exit code 137:</li> <li>Exit code 137, likely means your docker container used to much memory. You can adjust how much memory each process gets in the base.config file, however there may be some underlying configuration you need to perform for Docker to solve this issue.</li> </ul>"},{"location":"troubleshooting/FAQ/#checkm-fails","title":"CheckM fails","text":"<ul> <li>CheckM exit code 1, could not find concatenated.tree or concatentated.pplacer.json</li> <li>This is a sign that CheckM has run out of memory, make sure you are using your desired executor. You may need to adjust configuration settings.</li> </ul>"},{"location":"troubleshooting/FAQ/#quast-fails-with-a-read-only-error","title":"QUAST fails with a read-only error","text":"<ul> <li><code>[Errno 30] Read-only file system: '/usr/local/lib/python3.9/site-packages/quast_libs/gridss'</code></li> <li>This issue appears to be related to QUAST trying to download GRIDSS for structural variant detection and this action being incompatible with the container used to run QUAST. You may be able to resolve this be adding <code>--no-sv</code> as a QUAST command-line flag in Mikrokondo's <code>nextflow.config</code>, or by switching your container platform to singularity. Errors were observed with <code>apptainer version 1.2.3</code>, which were resolved by switching to singularity (<code>singularity-ce version 3.9.5</code> and <code>singularity-ce version a948062</code> resolved the issue).</li> </ul>"},{"location":"troubleshooting/FAQ/#ectyper-fails","title":"ECTyper fails","text":"<ul> <li>ECTyper makes uses of pythons temporary files which can result in issues on shared file systems e.g. <code>ntfs</code>. If you encounter issues try running the pipeline in a place where read and write permissions are more relaxed.</li> </ul>"},{"location":"troubleshooting/FAQ/#medaka-cannot-find-models","title":"Medaka cannot find models","text":"<ul> <li>It seems certain versions of nextflow affect the ability of medaka to find the required model passed to mikrokondo. Future iterations of mikrokondo will be updated to deal with this issue, and likely integrate dorado. If you have issues it is recommeneded to add <code>--skip_polishing true</code> to your command line, which will bypass medaka.</li> </ul>"},{"location":"troubleshooting/FAQ/#issues-with-groovy-script-modules-and-resume","title":"Issues with groovy script modules and resume","text":"<ul> <li>We have observed issues when resuming the pipeline when running a large number of samples (&gt;500) in HPC environments resulting in the pipeline getting stuck. If this happens to you regularly it is best to skip those processes. This primarily affects allele calling with locidex and StarAMR.</li> </ul>"},{"location":"usage/Utilities/","title":"Utilities","text":""},{"location":"usage/Utilities/#run-script","title":"Run script","text":"<p>The command line interface for Nextflow can become lengthy and tedious to type each time, due to the customization associated with routine pipeline runs, the lack of short form variable flags in Nextflow e.g. typing <code>--nanopore_chemisty</code> each time can be tedious.</p> <p>A run script skeleton has been provided in the <code>utils</code> folder of mikrokondo (<code>utils/mk_run.sh</code>). Please customize the script to make it fit your usage, if you have issues running your modified script make sure it is executable by adding running <code>chmod +x mk_run.sh</code>.</p>"},{"location":"usage/Utilities/#parameter-file-params-file","title":"Parameter file -params-file","text":"<p>You can also add a params file to the launch of Nextflow from the command line. More information is provided here</p>"},{"location":"usage/configuration/","title":"Configuration","text":""},{"location":"usage/configuration/#configuration-files-overview","title":"Configuration files overview","text":"<p>The following files contain configuration settings:</p> <ul> <li> <p><code>conf/base.config</code>: Where cpu, memory and time parameters can be set for the different workflow processes. You will likely need to adjust parameters within this file for your computing environment.</p> </li> <li> <p><code>conf/modules.config</code>: contains error strategy, output director structure and execution instruction parameters. It is unadvised to alter this file unless involved in pipeline development, or tuning to a system.</p> </li> <li> <p><code>conf/equivalent_taxa.json</code>: Contains a set of keys to arrays containing the query notes from the mash sketch denoting \"equivalent taxa\". This typically contains a list of organisms that are genetically similar but phenotypically distinct, as mobile elements or genome segments may be shared across organisms. e.g. Shigella and Escherichia</p> </li> <li> <p><code>nextflow.config</code>: contains default tool settings that tie to CLI options. These options can be directly set within the <code>params</code> section of this file in cases when a user has optimized their pipeline usage and has identified the flags they will use every time the pipeline is run.</p> </li> </ul>"},{"location":"usage/configuration/#base-configuration-confbaseconfig","title":"Base configuration (conf/base.config)","text":"<p>Within this file computing resources can be configured for each process. Mikrokondo uses labels to define resource requirements for each process, here are their definitions:</p> <ul> <li><code>process_single</code>: processes requiring only a single core and low memory (e.g., listing of directories).</li> <li><code>process_low</code>: processes that would typically run easily on a small laptop (e.g., staging of data in a Python script).</li> <li><code>process_medium</code>: processes that would typically run on a desktop computer equipped for playing newer video games (Memory or computationally intensive applications that can be parallelized, e.g., rendering, processing large files in memory or running BLAST).</li> <li><code>process_high</code>: processes that would typically run on a high performance desktop computer (Memory or computationally intensive application, e.g., performing de novo assembly or performing BLAST searches on large databases).</li> <li><code>process_long</code>: modifies/overwrites the amount of time allowed for any of the above processes to allow for certain jobs to take longer (e.g., performing de novo assembly with less computational resources or performing global alignments on divergent sequences).</li> <li><code>process_high_memory</code>: modifies/overwrites the amount of memory given to any process and grant significantly more memory to any process (Aids in metagenomic assembly or clustering of large datasets).</li> </ul> <p>For actual resource amounts allotted to each process definition, see the <code>conf/base.config</code> file Process-specific resource requirements section.</p>"},{"location":"usage/configuration/#hardcoded-tool-configuration-nextflowconfig","title":"Hardcoded tool configuration (nextflow.config)","text":"<p>All Command line arguments and defaults can be set and/or altered in the <code>nextflow.config</code> file, params section. For a full list of parameters to be altered please refer to the <code>nextflow.config</code> file in the repo. Some common arguments have been listed in the Common command line arguments section of the docs and further description of tool parameters can also be found in tool specific parameters.</p> <p>Example: if your laboratory typically sequences using Nanopore chemistry \"r1041_e82_400bps_hac_v4.2.0\", the following code would be substituted in the params section of the <code>nextflow.config</code> file:</p> <p><code>nanopore_chemistry = \"r1041_e82_400bps_hac_v4.2.0\" // Note the quotes around the value</code></p> <p>With this change, you would no longer need to explicitly state the nanopore chemistry as an extra CLI argument when running mikrokondo.</p>"},{"location":"usage/configuration/#quality-control-report-configuration","title":"Quality control report configuration","text":"<p>WARNING: Tread carefully here, as this will require modification of the <code>nextflow.config</code> file. Make sure you have saved a back up of your <code>nextflow.config</code> file before playing with these option</p>"},{"location":"usage/configuration/#qcreport-field-description","title":"QCReport field description","text":"<p>The section of interest is the <code>QCReport</code> fields in the params section of the <code>nextflow.config</code>. There are multiple sections with values that can be modified or you can add data for a different organism. The default values in the pipeline are set up for Illumina data so you may need to adjust settingS for Nanopore or Pacbio data.</p> <p>WARNING: It is best to only add <code>QCReport</code> values to mikrokondo, not remove existing ones as this may raise errors in the subtyping module.</p> <p>An example of the QCReport structure is shown below. With annotation describing the values.</p> <p>NOTE: The values below do not affect the running of the pipeline, these values only affect the final quality messages output by the pipeline.</p> <p>NOTE: The term JSON path is used below to indicate the ordered set of keys required to reach a particular value in an aggregated JSON file created by mikrokondo. For example if you wished to specify the JSON path to the ECTyper species identification in mikrokondo you would enter <code>[\"ECTyperSubtyping\", \"0\", \"Species\"]</code> as the JSON path which would point to a samples ECTyper speciation field in the following JSON structure from the <code>final_report.json</code> file generated by mikrokondo:</p> <pre><code>  \"ECTyperSubtyping\": {\n    \"0\": {\n      \"Species\": \"Escherichia coli\"\n    }\n  }\n</code></pre> <p>If you need to find a the JSON path required to point to a value, you can either look at the sample JSON output in <code>FinalReports/Aggregated/Json/final_report.json</code> file generated by mikrokondo to determine the required sequence of keys. However it is simpler to find the required JSON path value by looking in the <code>FinalReports/Sample/Json/final_report_flattened.json</code> file where you will see the key path listed and delimited by '.' e.g. <code>ECTyperSubtyping.0.Species</code> which would be re-written as <code>[\"ECTyperSubtyping, \"0\", \"Species\"]</code>.</p> <pre><code>QCReport {\n    escherichia // Generic top level name fo the field, it is name is technically arbitrary but it nice field name keeps things organized\n    {\n        search = \"Escherichia\" // The phrase that is searched for in the species_top_hit field mentioned above. The search is for containment so if you wanted to look for E.coli and E.albertii you could just set the value to \"Escherichia coli\" or \"Escherichia albertii\"\n        raw_average_quality = 30 // Minimum raw average quality of all bases in the sequencing data. This value is generated before the decontamination procedure.\n        min_n50 = 95000 // The minimum n50 value allowed from quast\n        max_n50 = 6000000 // The maximum n50 value allowed from quast\n        min_nr_contigs = 1 // the minimum number of contigs a sample is allowed to have, a value of 1 works as a sanity check\n        max_nr_contigs = 500 // The maximum number of contigs the organism in the search field is allowed to have. to many contigs could indicate a bad assembly or contamination\n        min_length = 4500000 // The minimum genome length allowed for the organism specified in the search field\n        max_length = 6000000 // The maximum genome length the organism in the search field is allowed to have\n        max_checkm_contamination = 3.0 // The maximum level of allowed contamination allowed by CheckM\n        min_average_coverage = 30 // The minimum average coverage allowed\n        // If you wish to make use of IDField and IDTool you will need to set both values\n        IDField = null // null|JSON path to relevant file results if null the mash or kraken2 results will be used\n        IDTool = null // null|tool name used to create file result if null mash or kraken2 will be written\n        // If PrimaryTypeID is set PrimaryTypeIDMethod must be as well\n        PrimaryTypeID: null|JSON Path, Optional primary type to have displayed e.g. serotype\n        PrimaryTypeIDMethod: null|String, Method used for Primary type e.g. ECTyper\n        // If SecondaryTypeID is set SecondaryTypeIDMethod must be as well\n        SecondaryTypeID: null|JSON Path, Secondary type information e.g. Clonal Complex\n        SecondaryTypeIDMethod: null|String, method used for secondary type information e.g. 7 Gene\n    }\n    // DO NOT REMOVE THE FALLTRHOUGH FIELD AS IT IS NEEDED TO CAPTURE OTHER ORGANISMS\n    fallthrough // The fallthrough field exist as a default value to capture organisms where no quality control data has been specified\n    {\n        search = \"No organism specific QC data available.\"\n        raw_average_quality = 30\n        min_n50 = null\n        max_n50 = null\n        min_nr_contigs = null\n        max_nr_contigs = null\n        min_length = null\n        max_length = null\n        max_checkm_contamination = 3.0\n        min_average_coverage = 30\n        IDField = null\n        IDTool = null\n        PrimaryTypeID = null\n        PrimaryTypeIDMethod = null\n        SecondaryTypeID = null\n        SecondaryTypeIDMethod = null\n    }\n}\n</code></pre>"},{"location":"usage/configuration/#example-adding-quality-control-data-for-salmonella","title":"Example adding quality control data for Salmonella","text":"<p>If you wanted to add quality control data for Salmonella you can start off by using the template below:</p> <pre><code>VAR_NAME { // Replace VAR name with the genus name of your sample, only use ASCII (a-zA-Z) alphabet characters in the name and replace spaces, punctuation and other special characters with underscores (_)\n    search = \"Search phrase\" // Search phrase for your species top_hit, Note the quotes\n    raw_average_quality = // 30 is a default value please change it as needed\n    min_n50 = // Set your minimum n50 value\n    max_n50 = // Set a maximum n50 value\n    min_nr_contigs = // Set a minimum number of contigs\n    max_nr_contigs = // The maximum number of contigs\n    min_length = // Set a minimum genome length\n    max_length = // set a maximum genome length\n    max_checkm_contamination = // Set a maximum level of contamination to use\n    min_average_coverage = // Set the minimum coverage value\n    IDField = null // Set a Json path found in the flattened report generated by mikrokondo to your tool result\n    IDTool = null // Set a custom name for the ID method you want to set\n}\n</code></pre> <p>For Salmonella I would fill in the values like so.</p> <pre><code>salmonella {\n    search = \"Salmonella\"\n    raw_average_quality = 30\n    min_n50 = 95000\n    max_n50 = 6000000\n    min_nr_contigs = 1\n    max_nr_contigs = 200\n    min_length = 4400000\n    max_length = 6000000\n    max_checkm_contamination = 3.0\n    min_average_coverage = 30\n    IDField = [params.sistr.report_tag, \"0\", \"Serovar\"]\n    IDTool = \"SISTR\"\n}\n</code></pre> <p>After having my values filled out, I can simply add them to the QCReport section in the <code>nextflow.config</code> file.</p> <pre><code>    QCReport {\n        escherichia {\n            search = \"Escherichia coli\"\n            raw_average_quality = 30\n            min_n50 = 95000\n            max_n50 = 6000000\n            min_nr_contigs = 1\n            max_nr_contigs = 500\n            min_length = 4500000\n            max_length = 6000000\n            max_checkm_contamination = 3.0\n            min_average_coverage = 30\n        } salmonella { // NOTE watch the opening and closing brackets\n            search = \"Salmonella\"\n            raw_average_quality = 30\n            min_n50 = 95000\n            max_n50 = 6000000\n            min_nr_contigs = 1\n            max_nr_contigs = 200\n            min_length = 4400000\n            max_length = 6000000\n            max_checkm_contamination = 3.0\n            min_average_coverage = 30\n        }\n        fallthrough {\n            search = \"No organism specific QC data available.\"\n            raw_average_quality = 30\n            min_n50 = null\n            max_n50 = null\n            min_nr_contigs = null\n            max_nr_contigs = null\n            min_length = null\n            max_length = null\n            max_checkm_contamination = 3.0\n            min_average_coverage = 30\n        }\n    }\n</code></pre>"},{"location":"usage/configuration/#quality-control-fields","title":"Quality Control Fields","text":"<p>This section affects the behavior of the final summary quality control messages and is noted in the <code>QCReportFields</code> within the <code>nextflow.config</code>. I would advise against manipulating this section unless you really know what you are doing.</p> <p>Each value in the QC report fields contains the following fields.</p> <ul> <li>Field name</li> <li>path: path to the information in the summary report JSON</li> <li>coerce_type: Type to be coerced too, can be a Float, Integer, or Bool</li> <li>compare_fields: A list of fields corresponding to fields in the <code>QCReport</code> section of the <code>nextflow.config</code>. If two values are specified it will be assumed you wish to check that a value is in between a range of values.</li> <li>comp_type: The comparison type specified, 'ge' for greater or equal, 'le' for less than or equal, 'bool' for true or false or 'range' for checking if a value is between two values.</li> <li>on: A boolean value for disabling a comparison</li> <li>low_msg: A message for if a value is less than its compared value (optional)</li> <li>high_msg: A message for if value is above a certain value (optional)</li> </ul> <p>An example of what these fields look like is:</p> <pre><code>QCReportFields {\n    raw_average_quality {\n        path = [params.raw_reads.report_tag, \"combined\", \"qual_mean\"]\n        coerce_type = 'Float'\n        compare_fields = ['raw_average_quality']\n        comp_type = \"ge\"\n        on = true\n        low_msg = \"Base quality is poor, resequencing is recommended.\"\n    }\n}\n</code></pre>"},{"location":"usage/configuration/#locidex-manifest-file","title":"Locidex Manifest File","text":"<p>Automated selection allele calling databases is supported within mikrokondo. This is accomplished with the help of Locidex itself, which offers a utility to generate a <code>manifest.json</code> file.</p> <p>The directory of a database set for Locidex contains the following structure as the <code>manifest.json</code> keeps track of the paths relative too the location of the manifest file itself:</p> <pre><code>--|\n  |- Database 1\n  |- Database 2\n  |- Database n\n  |- manifest.json\n</code></pre> <p>An example <code>manifest.json</code> file can be found in the mikrokondo test data sets here.</p> <p>Internally the <code>manifest.json</code> contains the following structure. Modifications to what <code>locidex manifest</code> outputs can be made as long as all fields populated. In the below example the <code>manifest.json</code> file generated by locidex has been modified to create two separate entries for Escherichia coli and Shigella.</p> <pre><code>{\n  \"Salmonella\": [\n    {\n      \"path\": \"wgmlst_salmonella\",\n      \"config\": {\n        \"db_name\": \"Salmonella\",\n        \"db_version\": \"1.0.0\",\n        \"db_date\": \"2024-03-17\",\n        \"db_author\": \"Tester\",\n        \"db_desc\": \"Salmonella Database\",\n        \"db_num_seqs\": 51251,\n        \"is_nucl\": true,\n        \"is_prot\": true,\n        \"nucleotide_db_name\": \"nucleotide\",\n        \"protein_db_name\": \"protein\"\n      }\n    }\n  ],\n  \"Escherichia coli\": [\n    {\n      \"path\": \"wgmlst_escherichia_shigella\",\n      \"config\": {\n        \"db_name\": \"EC and Shigella\",\n        \"db_version\": \"1.0.0\",\n        \"db_date\": \"2024-04-30\",\n        \"db_author\": \"Tester\",\n        \"db_desc\": \"Shigella and E.coli\",\n        \"db_num_seqs\": 57692,\n        \"is_nucl\": true,\n        \"is_prot\": true,\n        \"nucleotide_db_name\": \"nucleotide\",\n        \"protein_db_name\": \"protein\"\n      }\n    }\n  ],\n  \"Shigella\": [\n    {\n      \"path\": \"wgmlst_escherichia_shigella\",\n      \"config\": {\n        \"db_name\": \"EC and Shigella\",\n        \"db_version\": \"1.0.0\",\n        \"db_date\": \"2024-04-30\",\n        \"db_author\": \"Tester\",\n        \"db_desc\": \"Shigella and E.coli\",\n        \"db_num_seqs\": 57692,\n        \"is_nucl\": true,\n        \"is_prot\": true,\n        \"nucleotide_db_name\": \"nucleotide\",\n        \"protein_db_name\": \"protein\"\n      }\n    }\n  ],\n  \"Listeria Monocytogenes\": [\n    {\n      \"path\": \"wgmlst_listeria\",\n      \"config\": {\n        \"db_name\": \"Listeria Monocytogenes wgMLST\",\n        \"db_version\": \"1.0.0\",\n        \"db_date\": \"2024-04-16\",\n        \"db_author\": \"Tester\",\n        \"db_desc\": \"Listeria Monocytogenes wgMLST\",\n        \"db_num_seqs\": 22404,\n        \"is_nucl\": true,\n        \"is_prot\": true,\n        \"nucleotide_db_name\": \"nucleotide\",\n        \"protein_db_name\": \"protein\"\n      }\n    }\n  ]\n}\n</code></pre>"},{"location":"usage/configuration/#how-automated-selection-works","title":"How automated selection works.","text":"<p>Mikrokondo is able to identify the species that a sample represents internally, but in order to identify the correct WgMLST scheme to use for allele calling the top-level key in the <code>manifest.json</code> file must be a name that can be parsed from the speciation output of Mash or Kraken2 e.g. Salmonella enterica, Campylobacter_A anatolicus, Escherichia etc.</p> <p>Note: The database and organism names are not case sensitive.</p> <p>Mikrokondo will then be able to match the bacterial name outputs to what is in the <code>manifest.json</code>. In the following example below the three bacteria (Salmonella enterica, Campylobacter_A anatolicus, Escherichia coli) would all be matched to the correct scheme:</p> <pre><code>{\n  \"Salmonella\": [\n    ...\n  ],\n  \"Escherichia coli\": [\n    ...\n  ],\n  \"Campylobacter\": [\n    ...\n  ]\n}\n</code></pre> <p>This is because mikrokondo looks for the best exact match from the database names in the output species name. So spurious tokens like the <code>_A</code> in Campylobacter_A anatolicus would be removed and the <code>Campylobacter</code> database would be selected. For Salmonella, as the key value <code>Salmonella</code> overlaps entirely with the Salmonella of Salmonella enterica the Salmonella database would be selected. If There was a <code>Salmonella Enterica</code> database that would be selected over the generic <code>Salmonella</code> scheme. The <code>Escherichia coli</code> database would be selected for Escherichia coli as there they are a 100% match.</p>"},{"location":"usage/examples/","title":"Command Line Examples","text":"<p>Some example commands of running mikrokondo are provided below:</p>"},{"location":"usage/examples/#running-paired-end-illumina-data-skipping-bakta","title":"Running paired-end illumina data skipping Bakta","text":"<p><code>nextflow run main.nf --input sample_sheet.csv --skip_bakta true --platform illumina --outdir ../test_illumina -profile singularity -resume</code></p> <p>The above command would run paired-end Illumina data, using Singularity as a container service, using resume (e.g if picks up where the pipeline left off if being run again), skipping Bakta and outputting results in a folder called <code>test_illumina</code> one directory back from where the pipeline is run. Note: your sample sheet does not need to be called sample_sheet.csv</p>"},{"location":"usage/examples/#running-paired-end-illumina-data-using-kraken2-for-classifying-the-top-species-hit","title":"Running paired-end illumina data using Kraken2 for classifying the top species hit","text":"<p><code>nextflow run main.nf --input sample_sheet.csv --skip_bakta true --run_kraken true --platform illumina --outdir ../test_illumina_kraken -profile singularity -resume</code></p> <p>The above command would run paired-end Illumina data, using Singularity as a container service, using resume (e.g if picks up where the pipeline left off if being run again), skipping Bakta, using kraken2 to classify the species top hit and outputting results in a folder called <code>test_illumina_kraken</code> one directory back from where the pipeline is run. Note: your sample sheet does not need to be called sample_sheet.csv</p>"},{"location":"usage/examples/#running-nanopore-data","title":"Running nanopore data","text":"<p><code>nextflow run main.nf --input sample_sheet.csv --skip_ont_header_cleaning true --nanopore_chemistry r941_min_hac_g507 --platform nanopore --outdir ../test_nanopore -profile docker -resume</code></p> <p>The above command would run single-end Nanopore data using Docker as a container service, using resume (e.g if picks up where the pipeline left off if being run again), outputting data into a folder called <code>../test_nanopore</code> and skipping the process of verifying all Nanopore fastq data headers are unique. Note: your sample sheet does not need to be called sample_sheet.csv</p>"},{"location":"usage/examples/#running-a-hybrid-assembly-using-unicycler","title":"Running a hybrid assembly using Unicycler","text":"<p><code>nextflow run main.nf --input sample_sheet.csv --hybrid_unicycler true --nanopore_chemistry r941_min_hac_g507 --platform hybrid --outdir ../test_hybrid -profile apptainer -resume</code></p> <p>The above command would run single-end Nanopore and paired-end Illumina data using apptainer as a container service, using resume (e.g if picks up where the pipeline left off if being run again), outputting data into a folder called <code>../test_hybrid</code> and using Unicycler for assembly. Note: your sample sheet does not need to be called sample_sheet.csv</p>"},{"location":"usage/examples/#running-a-hybrid-assembly-without-unicycler","title":"Running a hybrid assembly without Unicycler","text":"<p><code>nextflow run main.nf --input sample_sheet.csv --platform hybrid --outdir ../test_hybrid -profile singularity -resume</code></p> <p>The above command would run single-end Nanopore and paired-end Illumina data using singularity as a container service, using resume (e.g if picks up where the pipeline left off if being run again), outputting data into a folder called <code>../test_hybrid</code>. Note: your sample sheet does not need to be called sample_sheet.csv</p>"},{"location":"usage/examples/#running-metagenomic-nanopore-data","title":"Running metagenomic Nanopore data","text":"<p><code>nextflow run main.nf --skip_depth_sampling true --input sample_sheet.csv --skip_polishing true --skip_bakta true --metagenomic_run true --nanopore_chemistry r941_prom_hac_g507 --platform nanopore --outdir ../test_nanopore_meta -profile singularity -resume</code></p> <p>The above command would run single-end Nanopore and paired-end Illumina data using singularity as a container service, using resume (e.g if picks up where the pipeline left off if being run again), outputting data into a folder call <code>../test_nanopore_meta</code>, all samples would be labeled treated as metagenomic, assembly polishing would be turned off and annotation of assemblies with Bakta would not be performed, depth sampling would not be performed either. Note: your sample sheet does not need to be called sample_sheet.csv</p>"},{"location":"usage/installation/","title":"Installation","text":""},{"location":"usage/installation/#dependencies","title":"Dependencies","text":"<ul> <li>Python (3.10&gt;=)</li> <li>Nextflow (22.10.1&gt;=)</li> <li>Container service (Docker, Singularity, Apptainer have been tested)</li> <li>The source code: <code>git clone https://github.com/phac-nml/mikrokondo.git</code></li> </ul> <p>Dependencies can be installed with Conda (e.g. Nextflow and Python).</p>"},{"location":"usage/installation/#to-install-mikrokondo","title":"To install mikrokondo","text":"<p>Once all dependencies are installed (see below for instructions), to download the pipeline run:</p> <p><code>git clone https://github.com/phac-nml/mikrokondo.git</code></p>"},{"location":"usage/installation/#installing-nextflow","title":"Installing Nextflow","text":"<p>Nextflow is required to run mikrokondo (requires Linux), and instructions for its installation can be found at either: Nextflow Home or  Nextflow Documentation</p>"},{"location":"usage/installation/#container-engine","title":"Container Engine","text":"<p>Nextflow and Mikrokondo require the use of containers to run the pipeline, such as: Docker, Singularity (now apptainer), podman, gitpod, sifter and charliecloud.</p> <p>NOTE: Singularity was adopted by the Linux Foundation and is now called Apptainer. Singularity still exists, however newer installs will likely use Apptainer.</p>"},{"location":"usage/installation/#docker-or-singularity","title":"Docker or Singularity?","text":"<p>Docker requires root privileges which can can make it a hassle to install on computing clusters, while there are workarounds, Apptainer/Singularity does not. Therefore, using Apptainer/Singularity is the recommended method for running the mikrokondo pipeline.</p>"},{"location":"usage/installation/#issues","title":"Issues","text":"<p>Containers are not perfect, below is a list of some issues you may face using containers in mikrokondo, fixes for each issue will be detailed here as they are identified.</p> <ul> <li>Exit code 137, usually means the docker container used to much memory.</li> </ul>"},{"location":"usage/installation/#resources-to-download","title":"Resources to download","text":"<ul> <li>GTDB Mash Sketch: required for speciation and determination when sample is metagenomic</li> <li>Decontamination Index: Required for decontamination of reads (this is a minimap2 index)</li> <li>Kraken2 std database: Required for binning of metagenomic data and is an alternative to using Mash for speciation</li> <li>Bakta database: Running Bakta is optional and there is a light database option, however the full one is recommended. You will have to unzip and un-tar the database for usage.</li> </ul>"},{"location":"usage/installation/#fields-to-update-with-resources","title":"Fields to update with resources","text":"<p>It is recommended to store the above resources within the <code>databases</code> folder in the mikrokondo folder, this allows for a simple update to the names of the database in <code>nextflow.config</code> rather than a need for a full path description.</p> <p>Below shows where to update database resources in the <code>params</code> section of the <code>nextflow.config</code> file:</p> <pre><code>// Bakta db path, note the quotation marks\nbakta_db = \"/PATH/TO/BAKTA/DB\"\n\n// Decontamination minimap2 index, note the quotation marks\ndehosting_idx = \"/PATH/TO/DECONTAMINATION/INDEX\"\n\n// kraken db path, not the quotation marks\nkraken2_db = \"/PATH/TO/KRAKEN/DATABASE/\"\n\n// GTDB Mash sketch, note the quotation marks\nmash_sketch = \"/PATH/TO/MASH/SKETCH/\"\n\n</code></pre>"},{"location":"usage/tool_params/","title":"Tool Specific Parameters","text":"<p>To access tool specific parameters from the command line you must use the dot operator. For organization and readability sake, the below documentation is nested to indicate where the dot operator is used. For example:</p> <pre><code>- quast\n    - min_contig_length NUM\n</code></pre> <p>Translates to <code>--quast.min_contig_length NUM</code> on the CLI.</p> <p>Note: Easily changed parameters are bolded. Sensible defaults are provided.</p>"},{"location":"usage/tool_params/#abricate","title":"Abricate","text":"<p>Screens contigs for antimicrobial and virulence genes. If you wish to use a different Abricate database you may need to update the container you use.</p> <ul> <li>abricate</li> <li>singularity: Abricate singularity container</li> <li>docker: Abricate docker container</li> <li>args: Can be a string of additional command line arguments to pass to abricate</li> <li>report_tag: determines the name of the Abricate output in the final summary file. Do not change this unless doing pipeline development.</li> <li>header_p: This field tells the report module that the Abricate output contains headers. Do not change this unless doing pipeline development.</li> </ul>"},{"location":"usage/tool_params/#raw-read-metrics","title":"Raw Read Metrics","text":"<p>A custom Python script that gathers quality metrics for each fastq file.</p> <ul> <li>raw_reads</li> <li>high_precision: When set to true, floating point precision of values output are accurate down to very small decimal places. Recommended to leave this setting as false (use the standard floats), it is much faster and having such precise decimal places is not needed for this module.</li> <li>report_tag: this field determines the name of the Raw Read Metric field in the final summary report. Do not change this unless doing pipeline development.</li> </ul>"},{"location":"usage/tool_params/#coreutils","title":"Coreutils","text":"<p>In cases where a process uses bash scripting only, Nextflow by default will utilize system binaries when they are available and no container is specified. For reproducibility, we have chosen to use containers in such cases. When a better container is available, you can direct the pipeline to use it via below commands:</p> <ul> <li>coreutils</li> <li>singularity: coreutils singularity container</li> <li>docker: coreutils docker container</li> </ul>"},{"location":"usage/tool_params/#python","title":"Python","text":"<p>Some scripts require Python3, therefore a well tested Python3 container is provided for reproducibility. However, as all the scripts within mikrokondo use only the standard library you can swap these containers to use any python interpreter version. For instance, swapping in pypy3 may result a massive performance boost from the scripts, though this is currently untested.</p> <ul> <li>python3</li> <li>singularity: Python3 singularity container</li> <li>docker: Python3 docker container</li> </ul>"},{"location":"usage/tool_params/#kat","title":"KAT","text":"<p>Kat was previously used to estimate genome size, however at the time of writing KAT appears to be only infrequently updated and newer versions would have issues running/sometimes giving an incorrect output due to failures in peak recognition. Therefore, KAT has been removed from the pipeline, It's code still remains but it will be removed in the future.</p>"},{"location":"usage/tool_params/#seqtk","title":"Seqtk","text":"<p>Seqtk is used for both the sub-sampling of reads and conversion of fasta files to fastq files in mikrokondo. The usage of seqtk to convert a fasta to a fastq is needed in certain typing tools requiring reads as input (this was a design decision to keep the pipeline generalizable).</p> <ul> <li>seqtk</li> <li>singularity: Singularity container for seqtk</li> <li>docker: Docker container for seqtk</li> <li>seed: A seed value for sub-sampling</li> <li>reads_ext: Extension of reads after sub-sampling, do not touch alter this unless doing pipeline development.</li> <li>assembly_fastq: Extension of the fastas after being converted to fastq files. Do not change this unless doing pipeline development.</li> <li>report_tag: Name of seqtk data in the final summary report. Do not change this unless doing pipeline development.</li> </ul>"},{"location":"usage/tool_params/#rasusa","title":"Rasusa","text":"<p>For long read data Rasusa is used for down sampling as it take read length into consideration when down sampling.</p> <ul> <li>rasusa</li> <li>singularity: singularity container for rasusa</li> <li>docker: docker container for rasusa</li> <li>seed: A seed value for sub-sampling</li> <li>reads_ext: The extension of the generated fastq files. Do not change this unless doing pipeline development.</li> </ul>"},{"location":"usage/tool_params/#fastp","title":"FastP","text":"<p>Fastp is fast and widely used program for gathering of read quality metrics, adapter trimming, read filtering and read trimming. FastP has extensive options for configuration which are detailed in their documentation, but sensible defaults have been set. Adapter trimming in Fastp is performed using overlap analysis, however if you do not trust this you can specify the sequencing adapters used directly in the additional arguments for Fastp.</p> <ul> <li>fastp</li> <li>singularity: singularity container for FastP</li> <li>docker: docker container for FastP</li> <li>fastq_ext: extension of the output Fastp trimmed reads, Do not change this unless doing pipeline development.</li> <li>html_ext: Extension of the html report output by fastp, Do not touch unless doing pipeline development.</li> <li>json_ext: Extension of json report output by FastP do not touch unless doing pipeline development.</li> <li>report_tag: Title of FastP data in the summary report.</li> <li>average_quality_e: If a read/read-pair quality is less than this value it is discarded. Can be set from the command line with <code>--fp_average_quality</code>.</li> <li>cut_tail_mean_quality: The mean quality threshold for a sliding window below which trailing bases are trimmed from the reads. Can be set from the command line with <code>--fp_cut_tail_mean_quality</code></li> <li>cut_tail_window_size: The window size to cut a tail with. Can be set from the command line with <code>--fp_cut_tail_window_size</code>.</li> <li>complexity_threshold: the threshold for low complexity filter. Can be set from the command line with <code>--fp_complexity_threshold</code>.</li> <li>qualified_quality_phred: the quality of a base to be qualified if filtering by unqualified bases. Can be set from the command line with <code>--fp_qualified_phred</code>.</li> <li>unqualified_percent_limit: The percent amount of bases that are allowed to be unqualified in a read. This parameter is affected by the above qualified_quality_phred parameter and can be specified from the command line with <code>--fp_unqualified_percent_limit</code>.</li> <li>polyg_min_len: The minimum length to detect a polyG tail. This value can be set from the command line with <code>--fp_polyg_min_len</code>.</li> <li>polyx_min_len: The minimum length to detect a polyX tail. This value can be set from the command line with <code>--fp_polyx_min_len</code>.</li> <li>illumina_length_min: The minimum read length to be allowed in illumina data. This value can be set from the command line with <code>--fp_illumina_length_min</code>.</li> <li>illumina_length_max: The maximum read length allowed for illumina data. This value can be set from the command line with <code>--fp_illumina_length_max</code>.</li> <li>single_end_length_min: the minimum read length allowed in Pacbio or Nanopore data. This value can be set from the command line with <code>--fp_single_end_length_min</code>.</li> <li>dedup_reads: A parameter to be turned on to allow for deduplication of reads. This value can be set from the command line with <code>--fp_dedup_reads</code>.</li> <li>illumina_args: The command string passed to Fastp when using illumina data, if you override this parameter other set parameters such as average_quality_e must be overridden as well as the command string will be passed to FastP as written</li> <li>single_end_args: The command string passed to FastP if single end data is used e.g. Pacbio or Nanopore data. If this option is overridden you must specify all parameters passed to Fastp as this string is passed to FastP as written.</li> <li>report_exclude_fields: Fields in the summary json to be excluded from the final aggregated report. Do not alter this field unless doing pipeline development</li> </ul>"},{"location":"usage/tool_params/#chopper","title":"Chopper","text":"<p>Chopper was originally used for trimming of Nanopore reads, but FastP was able to do the same work so Chopper is no longer used. Its code currently remains but it cannot be run in the pipeline.</p>"},{"location":"usage/tool_params/#flye","title":"Flye","text":"<p>Flye is used for assembly of Nanopore data.</p> <ul> <li>flye</li> <li>nanopore<ul> <li>raw: corresponds to the option in Flye of <code>--nano-raw</code></li> <li>corr: corresponds to the option in Flye of <code>--nano-corr</code></li> <li>hq: corresponds to the option in Flye of <code>--nano-hq</code></li> </ul> </li> <li>pacbio<ul> <li>raw: corresponds to the option in Flye of <code>--pacbio-raw</code></li> <li>corr: corresponds to the option in Flye of <code>--pacbio-corr</code></li> <li>hifi: corresponds to the option in Flye of <code>--pacbio-hifi</code></li> </ul> </li> <li>singularity: Singularity container for Flye</li> <li>docker: Docker container for Flye</li> <li>fasta_ext: The file extension for fasta files. Do not alter this field unless doing pipeline development</li> <li>gfa_ext: The file extension for gfa files. Do not alter this field unless doing pipeline development</li> <li>gv_ext: The file extension for gv files. Do not alter this field unless doing pipeline development</li> <li>txt_ext: the file extension for txt files. Do not alter this field unless doing pipeline development</li> <li>log_ext: the file extension for the Flye log files. Do not alter this field unless doing pipeline development</li> <li>json_ext: the file extension for the Flye json files. Do not alter this field unless doing pipeline development</li> <li>polishing_iterations: The number of polishing iterations for Flye.</li> <li>ext_args: Extra command line options to pass to Flye</li> </ul>"},{"location":"usage/tool_params/#spades","title":"Spades","text":"<p>Used for paired end read assembly</p> <ul> <li>spades</li> <li>singularity: Singularity container for spades</li> <li>docker: Docker container for spades</li> <li>scaffolds_ext: The file extension for the scaffolds file. Do not alter this field unless doing pipeline development</li> <li>contigs_ext: The file extension containing assembled contigs. Do not alter this field unless doing pipeline development</li> <li>transcripts_ext: The file extension for the assembled transcripts. Do not alter this field unless doing pipeline development</li> <li>assembly_graphs_ext: the file extension of the assembly graphs. Do not alter this field unless doing pipeline development</li> <li>log_ext: The file extension for the log files. Do not alter this field unless doing pipeline development</li> <li>outdir: The name of the output directory for assemblies. Do not alter this field unless doing pipeline development</li> </ul>"},{"location":"usage/tool_params/#fastqc","title":"FastQC","text":"<p>This is a default tool added to nf-core pipelines. This feature will likely be removed in the future but for those fond of it, the outputs of FastQC still remain.</p> <ul> <li>fastqc</li> <li>html_ext: The file extension of the fastqc html file. Do not alter this field unless doing pipeline development</li> <li>zip_ext: The file extension of the zipped FastQC outputs. Do not alter this field unless doing pipeline development</li> </ul>"},{"location":"usage/tool_params/#quast","title":"Quast","text":"<p>Quast is used to gather assembly metrics which automated quality control criteria are the applied too.</p> <ul> <li>quast</li> <li>singularity: Singularity container for quast.</li> <li>docker: Docker container for quast.</li> <li>suffix: The suffix attached to quast outputs. Do not alter this field unless doing pipeline development.</li> <li>report_base: The base term for output quast files to be used in reporting. Do not alter this field unless doing pipeline development.</li> <li>report_prefix: The prefix of the quast outputs to be used in reporting. Do not alter this field unless doing pipeline development.</li> <li>min_contig_length: The minimum length of for contigs to be used in quasts generation of metrics. Do not alter this field unless doing pipeline development. This argument can be set from the command line with <code>--qt_min_contig_length</code>.</li> <li>args: A command string to past to quast, altering this is unadvised as certain options may affect your reporting output. This string will be passed to quast verbatim. Do not alter this field unless doing pipeline development.</li> <li>header_p: This tells the pipeline that the Quast report outputs contains a header. Do not alter this field unless doing pipeline development.</li> </ul>"},{"location":"usage/tool_params/#quast-filter","title":"Quast Filter","text":"<p>Assemblies can be prevented from going into further analyses based on the Quast output. The options for the mentioned filter are listed here.</p> <ul> <li>quast_filter</li> <li>n50_field: The name of the field to search for and filter. Do not alter this field unless doing pipeline development.</li> <li>n50_value: The minimum value the field specified is allowed to contain.</li> <li>nr_contigs_field: The name of field in the Quast report to filter on. Do not alter this field unless doing pipeline development.</li> <li>nr_contigs_value: The minimum number of contigs an assembly must have to proceed further through the pipeline.</li> <li>sample_header: The column name in the Quast output containing the sample information. Do not alter this field unless doing pipeline development.</li> </ul>"},{"location":"usage/tool_params/#checkm","title":"CheckM","text":"<p>CheckM is used within the pipeline for assessing contamination in assemblies.</p> <ul> <li>checkm</li> <li>singularity: Singularity container containing CheckM</li> <li>docker: Docker container containing CheckM</li> <li>alignment_ext: Extension on the genes alignment within CheckM. Do not alter this field unless doing pipeline development.</li> <li>results_ext: The extension of the file containing the CheckM results. Do not alter this field unless doing pipeline development.</li> <li>tsv_ext: The extension containing the tsv results from CheckM. Do not alter this field unless doing pipeline development.</li> <li>folder_name: The name of the folder containing the outputs from CheckM. Do not alter this field unless doing pipeline development.</li> <li>gzip_ext: The compression extension for CheckM. Do not alter this field unless doing pipeline development.</li> <li>lineage_ms: The name of the lineages.ms file output by CheckM. Do not alter this field unless doing pipeline development.</li> <li>threads: The number of threads to use in CheckM. Do not alter this field unless doing pipeline development.</li> <li>report_tag: The name of the CheckM data in the summary report. Do not alter this field unless doing pipeline development.</li> <li>header_p: Denotes that the result used by the pipeline in generation of the summary report contains a header. Do not alter this field unless doing pipeline development.</li> </ul>"},{"location":"usage/tool_params/#kraken2","title":"Kraken2","text":"<p>Kraken2 can be used a substitute for mash in speciation of samples, and it is used to bin contigs of metagenomic samples.</p> <ul> <li>kraken</li> <li>singularity: Singularity container for the Kraken2.</li> <li>docker: Docker container for Kraken2.</li> <li>classified_suffix: Suffix for classified data from Kraken2. Do not alter this field unless doing pipeline development.</li> <li>unclassified_suffix: Suffix for unclassified data from Kraken2. Do not alter this field unless doing pipeline development.</li> <li>report_suffix: The name of the report output by Kraken2.</li> <li>output_suffix: The name of the output file from Kraken2. Do not alter this field unless doing pipeline development.</li> <li>tophit_level: The taxonomic level to classify a sample at. e.g. default is <code>S</code> for species but you could use <code>S1</code> or <code>F</code>.</li> <li>save_output_fastqs: Option to save the output fastq files from Kraken2. Do not alter this field unless doing pipeline development.</li> <li>save_read_assignments: Option to save how Kraken2 assigns reads. Do not alter this field unless doing pipeline development.</li> <li>run_kraken_quick: This option can be set to <code>true</code> if one wishes to run Kraken2 in quick mode.</li> <li>report_tag: The name of the Kraken2 data in the final report. Do not alter this field unless doing pipeline development.</li> <li>header_p: Tell the pipeline that the file used for reporting does or does not contain header data. Do not alter this field unless doing pipeline development.</li> <li>headers: A list of headers in the Kraken2 report. Do not alter this field unless doing pipeline development.</li> </ul>"},{"location":"usage/tool_params/#seven-gene-mlst","title":"Seven Gene MLST","text":"<p>Run Torsten Seemann's seven gene MLST program.</p> <ul> <li>mlst</li> <li>singularity: Singularity container for mlst.</li> <li>docker: Docker container for mlst.</li> <li>args: Additional arguments to pass to mlst.</li> <li>tsv_ext: Extension of the mlst tabular file. Do not alter this field unless doing pipeline development.</li> <li>json_ext: Extension of the mlst output JSON file. Do not alter this field unless doing pipeline development.</li> <li>report_tag: Name of the data outputs in the final report. Do not alter this field unless doing pipeline development.</li> </ul>"},{"location":"usage/tool_params/#mash","title":"Mash","text":"<p>Mash is used repeatedly throughout the pipeline for estimation of genome size from reads, contamination detection and for determining the final species of an assembly.</p> <ul> <li>mash</li> <li>singularity: Singularity container for mash.</li> <li>docker: Docker container for mash.</li> <li>mash_ext: Extension of the mash screen file. Do not alter this field unless doing pipeline development.</li> <li>output_reads_ext: Extension of mash outputs when run on reads. Do not alter this field unless doing pipeline development.</li> <li>output_taxa_ext: Extension of mash output when run on contigs. Do not alter this field unless doing pipeline development.</li> <li>mash_sketch: The GTDB sketch used by the pipeline, this sketch is special as it contains the taxonomic paths in the classification step of the pipeline. It can as of 2023-10-05 be found here: https://zenodo.org/record/8408361</li> <li>sketch_ext: File extension of a mash sketch. Do not alter this field unless doing pipeline development.</li> <li>json_ext: File extension of json data output by Mash. Do not alter this field unless doing pipeline development.</li> <li>sketch_kmer_size: The size of the kmers used in the sketching in genome size estimation.</li> <li>min_kmer: The minimum number of kmer copies required to pass the noise filter. this value is used in estimation of genome size from reads. The default value is 10 as it seems to work well for Illumina data. This value can be set from the command line by setting <code>--mh_min_kmer</code>.</li> <li>final_sketch_name: to be removed This parameter was originally part of a subworkflow included in the pipeline for generation of the GTDB sketch. But this has been removed and replaced with scripting.</li> <li>report_tag: Report tag for Mash in the summary report. Do not alter this field unless doing pipeline development.</li> <li>header_p: Tells the pipeline if the output data contains headers. Do not alter this field unless doing pipeline development.</li> <li>headers: A list of the headers the output of mash should contain. Do not alter this field unless doing pipeline development.</li> </ul>"},{"location":"usage/tool_params/#mash-meta","title":"Mash Meta","text":"<p>This process is used to determine if a sample is metagenomic or not.</p> <ul> <li>mash_meta.</li> <li>report_tag: The name of this output field in the summary report. Do not alter this field unless doing pipeline development.</li> </ul>"},{"location":"usage/tool_params/#top_hit_species","title":"top_hit_species:","text":"<p>As Kraken2 of Mash can be used for determining the species present in the pipeline, the share a common report tag.</p> <ul> <li>top_hig_species</li> <li>report_tag: The name of the determined species in the final report. Do not alter this field unless doing pipeline development.</li> </ul>"},{"location":"usage/tool_params/#contamination-removal","title":"Contamination Removal","text":"<p>This step is used to remove contaminants from read data, it exists to perform dehosting, and removal of kitomes.</p> <ul> <li>r_contaminants</li> <li>singularity: Singularity container used to perform dehosting, this container contains minimap2 and samtools.</li> <li>docker: Docker container used to perform dehosting, this container contains minimap2 and samtools.</li> <li>phix_fa: The path to file containing the phiX fasta.</li> <li>homo_sapiens_fa: The path to file containing the human genomes fasta.</li> <li>pacbio_mg: The path to file containing the pacbio sequencing control.</li> <li>output_ext: The extension of the deconned fastq files. Do not alter this field unless doing pipeline development.</li> <li>mega_mm2_idx: The path to the minimap2 index used for dehosting. Do not alter this field unless doing pipeline development.</li> <li>mm2_illumina: The arguments passed to minimap2 for Illumina data. Do not alter this field unless doing pipeline development.</li> <li>mm2_pac: The arguments passed to minimap2 for Pacbio Data. Do not alter this field unless doing pipeline development.</li> <li>mm2_ont: The arguments passed to minimap2 for Nanopore data. Do not alter this field unless doing pipeline development.</li> <li>samtools_output_ext: The extension of the output from samtools. Do not alter this field unless doing pipeline development.</li> <li>samtools_singletons_ext: The extension of singleton reads from samtools. Do not alter this field unless doing pipeline development.</li> <li>output_ext: The name of the files output from samtools. Do not alter this field unless doing pipeline development.</li> <li>output_dir: The directory where deconned reads are placed. Do not alter this field unless doing pipeline development.</li> </ul>"},{"location":"usage/tool_params/#minimap2","title":"Minimap2","text":"<p>Minimap2 is used frequently throughout the pipeline for decontamination and mapping reads back to assemblies for polishing.</p> <ul> <li>minimap2</li> <li>singularity: The singularity container for minimap2, the same one is used for contamination removal.</li> <li>docker: The Docker container for minimap2, the same one is used for contamination removal.</li> <li>index_outdir: The directory where created indices are output. Do not alter this field unless doing pipeline development.</li> <li>index_ext: The file extension of create indices. Do not alter this field unless doing pipeline development.</li> </ul>"},{"location":"usage/tool_params/#samtools","title":"Samtools","text":"<p>Samtools is used for sam to bam conversion in the pipeline.</p> <ul> <li>samtools</li> <li>singularity: The Singularity container containing samtools, the same container is used as the one in contamination removal.</li> <li>docker: The Docker container containing samtools, the same container is used as the on in contamination removal.</li> <li>bam_ext: The extension of the bam file from samtools. Do not alter this field unless doing pipeline development.</li> <li>bai_ext: the extension of the bam index from samtools. Do not alter this field unless doing pipeline development.</li> </ul>"},{"location":"usage/tool_params/#racon","title":"Racon","text":"<p>Racon is used as a first pass for polishing assemblies.</p> <ul> <li>racon</li> <li>singularity: The Singularity container containing racon.</li> <li>docker: The Docker container containing racon.</li> <li>consensus_suffix: The suffix for racons outputs. Do not alter this field unless doing pipeline development.</li> <li>consensus_ext: The file extension for the racon consensus sequence. Do not alter this field unless doing pipeline development.</li> <li>outdir: The directory containing the polished sequences. Do not alter this field unless doing pipeline development.</li> </ul>"},{"location":"usage/tool_params/#pilon","title":"Pilon","text":"<p>Pilon was added to the pipeline, but it is run iteratively which at the time of writing this pipeline was not well supported in Nextflow so a separate script and containers are provided to utilize Pilon. The code for Pilon remains in the pipeline so that when able to do so easily, iterative Pilon polishing can be integrated directly into the pipeline.</p>"},{"location":"usage/tool_params/#pilon-iterative-polishing","title":"Pilon Iterative Polishing","text":"<p>This process is a wrapper around minimap2, samtools and Pilon for iterative polishing containers are built but if you ever have problems with this step, disabling polishing will fix your issue (at the cost of polishing).</p> <ul> <li>pilon_iterative</li> <li>singularity: The container containing the iterative pilon program. If you ever have issues with the singularity image you can use the Docker image as Nextflow will automatically convert the docker image into a singularity image.</li> <li>docker: The Docker container for the Pilon iterative polisher.</li> <li>outdir: The directory where polished data is output. Do not alter this field unless doing pipeline development.</li> <li>fasta_ext: File extension for the fasta to be polished. Do not alter this field unless doing pipeline development.</li> <li>fasta_outdir: The output directory name for the polished fastas. Do not alter this field unless doing pipeline development.</li> <li>vcf_ext: File extension for the VCF output by Pilon. Do not alter this field unless doing pipeline development.</li> <li>vcf_outdir: output directory containing the VCF files from Pilon. Do not alter this field unless doing pipeline development.</li> <li>bam_ext: Bam file extension. Do not alter this field unless doing pipeline development.</li> <li>bai_ext: Bam index file extension. Do not alter this field unless doing pipeline development.</li> <li>changes_ext: File extensions for the pilon output containing the changes applied to the assembly. Do not alter this field unless doing pipeline development.</li> <li>changes_outdir: The output directory for the pilon changes. Do not alter this field unless doing pipeline development.</li> <li>max_memory_multiplier: On failure this program will try again with more memory, the multiplier is the factor that the amount of memory passed to the program will be increased by. Do not alter this field unless doing pipeline development.</li> <li>max_polishing_illumina: Number of iterations for polishing an illumina assembly with illumina reads.</li> <li>max_polishing_nanopore: Number of iterations to polish a Nanopore assembly with (will use illumina reads if provided).</li> <li>max_polishing_pacbio: Number iterations to polish assembly with (will use illumina reads if provided).</li> </ul>"},{"location":"usage/tool_params/#medaka-polishing","title":"Medaka Polishing","text":"<p>Medaka is used for polishing of Nanopore assemblies, make sure you specify a medaka model when using the pipeline so the correct settings are applied. If you have issues with Medaka running, try disabling resume or alternatively disable polishing as Medaka can be troublesome to run.</p> <ul> <li>medaka</li> <li>singularity: Singularity container with Medaka.</li> <li>docker: Docker container with Medaka.</li> <li>model: This parameter will be auto filled with the model specified at the top level by the <code>nanopore_chemistry</code> option. Do not alter this field unless doing pipeline development.</li> <li>fasta_ext: Polished fasta output. Do not alter this field unless doing pipeline development.</li> <li>batch_size: The batch size passed to medaka, this can improve performance. Do not alter this field unless doing pipeline development.</li> </ul>"},{"location":"usage/tool_params/#unicycler","title":"Unicycler","text":"<p>Unicycler is an option provided for hybrid assembly, it is a great option and outputs an excellent assembly but it requires A lot of resources. Which is why the alternate hybrid assembly option using Flye-&gt;Racon-&gt;Pilon is available. As well there can be a fairly cryptic Spades error generated by Unicycler that usually relates to memory usage, it will typically say something involving <code>tputs</code>.</p> <ul> <li>unicycler</li> <li>singularity: The Singularity container containing Unicycler.</li> <li>docker: The Docker container containing Unicycler.</li> <li>scaffolds_ext: The scaffolds file extension output by unicycler. Do not alter this field unless doing pipeline development.</li> <li>assembly_ext: The assembly extension output by Unicycler. Do not alter this field unless doing pipeline development.</li> <li>log_ext: The log file output by Unicycler. Do not alter this field unless doing pipeline development.</li> <li>outdir: The output directory the Unicycler data is sent to. Do not alter this field unless doing pipeline development.</li> <li>mem_modifier: Specifies a high amount of memory for Unicycler to prevent a common spades error that is fairly cryptic. Do not alter this field unless doing pipeline development.</li> <li>threads_increase_factor: Factor to increase the number of threads passed to Unicycler. Do not alter this field unless doing pipeline development.</li> </ul>"},{"location":"usage/tool_params/#mob-suite-recon","title":"Mob-suite Recon","text":"<p>mob-suite recon provides annotation of plasmids in the assembly data.</p> <ul> <li>mobsuite_recon</li> <li>singularity: The singularity container containing mob-suite recon.</li> <li>docker: The Docker container containing mob-suite recon.</li> <li>args: Additional arguments to pass to mobsuite.</li> <li>fasta_ext: The file extension for FASTAs. Do not alter this field unless doing pipeline development.</li> <li>results_ext: The file extension for results in mob-suite. Do not alter this field unless doing pipeline development.</li> <li>mob_results_file: The final results to be included in the final report by mob-suite. Do not alter this field unless doing pipeline development.</li> <li>report_tag: The field name of mob-suite data in the final report. Do not alter this field unless doing pipeline development.</li> <li>header_p: Default is <code>true</code> and indicates that the results output by mob-suite contains a header. Do not alter this field unless doing pipeline development.</li> </ul>"},{"location":"usage/tool_params/#staramr","title":"StarAMR","text":"<p>StarAMR provides annotation of antimicrobial resistance genes within your data. The process will alter FASTA headers of input files to ensure the header length &lt;50 characters long.</p> <ul> <li>staramr</li> <li>singularity: The singularity container containing staramr.</li> <li>docker: The Docker container containing StarAMR.</li> <li>db: The database for StarAMR. The default value of <code>null</code> tells the pipeline to use the database included in the StarAMR container. However you can specify a path to a valid StarAMR database and use that instead.</li> <li>tsv_ext: File extension of the reports from StarAMR. Do not alter this field unless doing pipeline development.</li> <li>txt_ext: File extension of the text reports from StarAMR. Do not alter this field unless doing pipeline development.</li> <li>xlsx_ext: File extension of the excel spread sheet from StarAMR. Do not alter this field unless doing pipeline development.</li> <li>args: Additional arguments to pass to StarAMR. Do not alter this field unless doing pipeline development.</li> <li>point_finder_dbs: A list containing the valid databases StarAMR supports for pointfinder. The way they are structured matches what StarAMR needs for input. Do not alter this field unless doing pipeline development.</li> <li>report_tag: The field name of StarAMR in the final summary report. Do not alter this field unless doing pipeline development.</li> <li>header_p: Indicates the final report from StarAMR contains a header line. Do not alter this field unless doing pipeline development.</li> </ul>"},{"location":"usage/tool_params/#bakta","title":"Bakta","text":"<p>Bakta is used to provide annotation of genomes, it is very reliable but it can be slow.</p> <ul> <li>bakta</li> <li>singularity: The singularity container containing Bakta.</li> <li>docker: The Docker container containing Bakta.</li> <li>db: the path where the downloaded Bakta database should be downloaded. This can be set from the command line using the argument <code>--bakta_db</code>.</li> <li>output_dir: The name of the folder where Bakta data is saved too. Do not alter this field unless doing pipeline development.</li> <li>embl_ext: File extension of embl file. Do not alter this field unless doing pipeline development.</li> <li>faa_ext: File extension of faa file. Do not alter this field unless doing pipeline development.</li> <li>ffn_ext: File extension of the ffn file. Do not alter this field unless doing pipeline development.</li> <li>fna_ext: File extension of the fna file. Do not alter this field unless doing pipeline development.</li> <li>gbff_ext: File extension of gbff file. Do not alter this field unless doing pipeline development.</li> <li>gff_ext: File extension of GFF file. Do not alter this field unless doing pipeline development.</li> <li>threads: Number of threads for Bakta to use, remember more is not always better. Do not alter this field unless doing pipeline development.</li> <li>hypotheticals_tsv_ext: File extension for hypothetical genes. Do not alter this field unless doing pipeline development.</li> <li>hypotheticals_faa_ext: File extension of hypothetical genes fasta. Do not alter this field unless doing pipeline development.</li> <li>tsv_ext: The file extension of the final bakta tsv report. Do not alter this field unless doing pipeline development.</li> <li>txt_ext: The file extension of the txt report. Do not alter this field unless doing pipeline development.</li> <li>min_contig_length: The minimum contig length to be annotated by Bakta. This can be set from the command line using the argument <code>--ba_min_contig_length</code>.</li> </ul>"},{"location":"usage/tool_params/#bandage","title":"Bandage","text":"<p>Bandage is included to make bandage plots of the initial assemblies e.g. Spades, Flye or Unicycler. These images can be useful in determining the quality of an assembly.</p> <ul> <li>bandage</li> <li>singularity: The path to the singularity image containing bandage.</li> <li>docker: The path to the docker file containing bandage.</li> <li>svg_ext: The extension of the SVG file created by bandage. Do not alter this field unless doing pipeline development.</li> <li>outdir: The output directory of the bandage images.</li> </ul>"},{"location":"usage/tool_params/#subtyping-report","title":"Subtyping Report","text":"<p>All sub typing report tools contain a common report tag so that they can be identified by the program.</p> <ul> <li>subtyping_report</li> <li>report_tag: Subtyping report name. Do not alter this field unless doing pipeline development.</li> </ul>"},{"location":"usage/tool_params/#ectyper","title":"ECTyper","text":"<p>ECTyper is used to perform in-silico typing of Escherichia and is automatically triggered by the pipeline.</p> <ul> <li>ectyper</li> <li>singularity: The path to the singularity container containing ECTyper.</li> <li>docker: The path to the Docker container containing ECTyper.</li> <li>log_ext: File extension of the ECTyper log file. Do not alter this field unless doing pipeline development.</li> <li>tsv_ext: File extension of the ECTyper text file. Do not alter this field unless doing pipeline development.</li> <li>txt_ext: Text file extension of ECTyper output. Do not alter this field unless doing pipeline development.</li> <li>report_tag: Report tag for ECTyper data. Do not alter this field unless doing pipeline development.</li> <li>header_p: denotes if the table output from ECTyper contains a header. Do not alter this field unless doing pipeline development.</li> <li>ec_opid`: The minimum percent identity to determine an O antigens presence, It must be an integer.</li> <li>ec_opcov: The minimum percent coverage of O antigen, It must be an integer.</li> <li>ec_hpid: The minimum percent identity to determine an H antigens presence, It must be an integer.</li> <li>ec_hcov: The minimum percent coverage of the H antigen, It must be an integer.</li> <li>ec_enable_verification: A boolean value to enable species verification in ECTyper.</li> </ul>"},{"location":"usage/tool_params/#kleborate","title":"Kleborate","text":"<p>Kleborate performs automatic typing of Kelbsiella.</p> <ul> <li>kleborate</li> <li>singularity: The path to the singularity container containing Kleborate.</li> <li>docker: The path to the docker container containing Kleborate.</li> <li>txt_ext: The subtyping report tag for Kleborate. Do not alter this field unless doing pipeline development.</li> <li>report_tag: The report tag for Kleborate. Do not alter this field unless doing pipeline development.</li> <li>header_p: Denotes the Kleborate table contains a header. Do not alter this field unless doing pipeline development.</li> </ul>"},{"location":"usage/tool_params/#spatyper","title":"Spatyper","text":"<p>Performs typing of Staphylococcus species.</p> <ul> <li>spatyper</li> <li>singularity: The path to the singularity container containing Spatyper.</li> <li>docker: The path to docker container containing Spatyper.</li> <li>tsv_ext: The file extension of the Spatyper output. Do not alter this field unless doing pipeline development.</li> <li>report_tag: The report tag for Spatyper. Do not alter this field unless doing pipeline development.</li> <li>header_p: denotes whether or not the output table contains a header. Do not alter this field unless doing pipeline development.</li> <li>repeats: An optional file specifying repeats can be passed to Spatyper.</li> <li>repeat_order: An optional file containing a repeat order to pass to Spatyper.</li> </ul>"},{"location":"usage/tool_params/#sistr","title":"SISTR","text":"<p>In-silico Salmonella serotype prediction.</p> <ul> <li>sistr</li> <li>singularity: The path to the singularity container containing SISTR.</li> <li>docker: The path to the Docker container containing SISTR.</li> <li>tsv_ext: The file extension of the SISTR output. Do not alter this field unless doing pipeline development.</li> <li>allele_fasta_ext: The extension of the alleles identified by SISTR. Do not alter this field unless doing pipeline development.</li> <li>allele_json_ext: The extension to the output JSON file from SISTR. Do not alter this field unless doing pipeline development.</li> <li>cgmlst_tag: The extension of the CGMLST file from SISTR. Do not alter this field unless doing pipeline development.</li> <li>report_tag: The report tag for SISTR. Do not alter this field unless doing pipeline development.</li> <li>header_p: Denotes whether or not the output table contains a header. Do not alter this field unless doing pipeline development.</li> </ul>"},{"location":"usage/tool_params/#lissero","title":"Lissero","text":"<p>in-silico Listeria typing.</p> <ul> <li>lissero</li> <li>singularity: The path to the singularity container containing Lissero.</li> <li>docker: The path to the docker container containing Lissero.</li> <li>tsv_ext: The file extension of the Lissero output. Do not alter this field unless doing pipeline development.</li> <li>report_tag: The report tag for Lissero. Do not alter this field unless doing pipeline development.</li> <li>header_p: Denotes if the output table of Lissero contains a header. Do not alter this field unless doing pipeline development.</li> </ul>"},{"location":"usage/tool_params/#shigeifinder","title":"Shigeifinder","text":"<p>in-silico Shigella typing.</p> <p>NOTE: It is unlikely this subtyper will be triggered as GTDB has merged E.coli and Shigella in an updated sketch. An updated version of ECTyper will be released soon to address the shortfalls of this sketch. If you are relying on Shigella detection add <code>--run_kraken true</code> to your command line or update the value in the <code>.nextflow.config</code> as Kraken2 (while slower) can still detect Shigella.</p> <ul> <li>shigeifinder</li> <li>singularity: The Singularity container containing Shigeifinder.</li> <li>docker: The path to the Docker container containing Shigeifinder.</li> <li>container_version: The version number to be updated with the containers as Shigeifinder does not currently have a version number tracked in the command.</li> <li>tsv_ext: Extension of output report.</li> <li>report_tag: The name of the output report for shigeifinder.</li> <li>header_p: Denotes that the output from Shigeifinder includes header values.</li> </ul>"},{"location":"usage/tool_params/#shigatyper-replaced-with-shigeifinder","title":"Shigatyper (Replaced with Shigeifinder)","text":"<p>Code still remains but it will likely be removed later on.</p> <ul> <li>shigatyper</li> <li>singularity: The Singularity container containing Shigatyper.</li> <li>docker: The path to the Docker container containing Shigatyper.</li> <li>tsv_ext: The tsv file extension. Do not alter this field unless doing pipeline development.</li> <li>report_tag: The report tag for Shigatyper. Do not alter this field unless doing pipeline development.</li> <li>header_p: Denotes if the report output contains a header. Do not alter this field unless doing pipeline development.</li> </ul>"},{"location":"usage/tool_params/#kraken2-contig-binning","title":"Kraken2 Contig Binning","text":"<p>Bins contigs based on the Kraken2 output for contaminated/metagenomic samples. This is implemented by using a custom script.</p> <ul> <li>kraken_bin</li> <li>taxonomic_level: The taxonomic level to bin contigs at. Binning at species level is not recommended the default is to bin at a genus level which is species by a character of <code>G</code>. To bin at a higher level such as family you would specify <code>F</code>.</li> <li>fasta_ext: The extension of the fasta files output. Do not alter this field unless doing pipeline development.</li> </ul>"},{"location":"usage/tool_params/#locidex-allele-calling","title":"Locidex (Allele Calling)","text":"<p>Parameters for use of locidex in allele calling.</p> <ul> <li>Locidex</li> <li>singularity: The Singularity container containing Locidex.</li> <li>docker: The path to the Docker container containing Locidex.</li> <li>private_repository: The path to the Docker container containing Locidex in a private repository (this helps in cloud execution environments).</li> <li>min_evalue = See <code>--lx_min_evalue</code>.</li> <li>min_dna_len = See <code>--lx_min_dna_len</code>.</li> <li>min_aa_len = See <code>--lx_min_aa_len</code>.</li> <li>max_dna_len = See <code>--lx_max_dna_len</code>.</li> <li>max_aa_len = See <code>--lx_max_aa_len</code>.</li> <li>min_dna_ident = See <code>--lx_min_dna_ident</code>.</li> <li>min_aa_ident = See <code>--lx_min_aa_ident</code>.</li> <li>min_dna_match_cov = See <code>--lx_min_dna_match_cov</code>.</li> <li>min_aa_match_cov = See <code>--lx_min_aa_match_cov</code></li> <li>max_target_seqs = See <code>--lx_max_target_seqs</code>.</li> <li>extraction_mode = See <code>--lx_extraction_mode</code>.</li> <li>report_mode = See <code>--lx_report_mode</code>.</li> <li>report_prop = See <code>--lx_report_prop</code>.</li> <li>report_max_ambig = See <code>--lx_report_max_ambig</code>.</li> <li>report_max_stop = See <code>--lx_report_max_stop</code>.</li> <li>allele_database = See <code>--lx_allele_database</code>.</li> <li>date_format_string: The date format used in parsing the locidex <code>manifest.json</code> file. Do not alter this field unless doing pipeline development.</li> <li>manifest_db_path: Do not alter this field unless doing pipeline development.</li> <li>manifest_config_key: The name of key holding config data. Do not alter this field unless doing pipeline development.</li> <li>manifest_config_name: The name field to use in the locidex <code>manifest.json</code> file for db identification. Do not alter this field unless doing pipeline development.</li> <li>manifest_config_version: Config key field containing the version information for locidex. Do not alter this field unless doing pipeline development.</li> <li>manifest_name: The name of the <code>manifest.json</code> file for locidex. Do not alter this field unless doing pipeline development.</li> <li>config_data_file: The name of the locidex database file containing config information. Do not alter this field unless doing pipeline development.</li> <li>database_config_value_date: Name of the field containing the date in the locidex <code>manifest.json</code>. Do not alter this field unless doing pipeline development.</li> <li>extracted_seqs_suffix: Extracted sequences file suffix. Do not alter this field unless doing pipeline development.</li> <li>seq_store_suffix: Seq store suffix. Do not alter this field unless doing pipeline development.</li> <li>gbk_suffix: Extension name of the generated GBK file. Do not alter this field unless doing pipeline development.</li> <li>extraction_dir: Directory name of the locidex extract outputs. Do not alter this field unless doing pipeline development.</li> <li>report_suffix: Report suffix of the locidex outputs. Do not alter this field unless doing pipeline development.</li> <li>db_config_output_name: Output name of the selected database used for locidex. Do not alter this field unless doing pipeline development.</li> <li>report_tag: The report tag for Locidex Report. Do not alter this field unless doing pipeline development.</li> </ul>"},{"location":"usage/tool_params/#locidex-summary","title":"Locidex Summary","text":"<p>The information used in creating a summary of the locidex outputs.</p> <ul> <li>locidex_summary</li> <li>report_tag: The report tag for the locidex summary. Do not alter this field unless doing pipeline development.</li> <li>data_key: The field containing the relevant data to summarize. Do not alter this field unless doing pipeline development.</li> <li>data_profile_key: The key containing the profile information. Do not alter this field unless doing pipeline development.</li> <li>data_sample_key: The name of the key containing the sample info. Do not alter this field unless doing pipeline development.</li> <li>missing_allele_value: The field used for the missing allele value. Do not alter this field unless doing pipeline development.</li> <li>reportable_alleles: A list of alleles to show their presence or absence of in the final output.</li> <li>report_exclude_fields: Fields to exclude from the final summary report. Do not alter this field unless doing pipeline development.</li> </ul>"},{"location":"usage/usage/","title":"Running MikroKondo","text":""},{"location":"usage/usage/#usage","title":"Usage","text":"<p>MikroKondo can be run like most other nextflow pipelines. The most basic usage is as follows: <code>nextflow run main.nf --input PATH_TO_SAMPLE_SHEET --outdir OUTPUT_DIR --platform SEQUENCING_PLATFORM  -profile CONTAINER_TYPE</code></p> <p>Many parameters can be altered or accessed from the command line. For a full list of parameters to be altered please refer to the <code>nextflow.config</code> file in the repo.</p>"},{"location":"usage/usage/#input","title":"Input","text":"<p>This pipeline requires the following as input:</p>"},{"location":"usage/usage/#sample-files-gzip","title":"Sample files (gzip)","text":"<p>This pipeline requires sample files to be gzipped (symlinks may be problematic).</p>"},{"location":"usage/usage/#samplesheet-csv","title":"Samplesheet (CSV)","text":"<p>Mikrokondo requires a sample sheet to be run. This FOFN (file of file names) contains the samples names and allows a user to combine read-sets based on that name if provided. The sample-sheet can utilize the following header fields:</p> <ul> <li>sample</li> <li>fastq_1</li> <li>fastq_2</li> <li>long_reads</li> <li>assembly</li> </ul> <p>Note: Illegal characters (e.g. characters that match the expression [^A-Za-z0-9_-] ) in the sample name will be replaced with underscores.</p> <p>Example layouts for different sample-sheets include:</p> <p>Illumina paired-end data</p> sample fastq_1 fastq_2 sample path_to_forward_reads path_to_reversed_reads <p>Nanopore</p> sample long_reads sample path_to_reads <p>Hybrid Assembly</p> sample fastq_1 fastq_2 long_reads sample path_to_forward_reads path_to_reversed_reads path_to_long_reads <p>Starting with assembly only</p> sample assembly sample path_to_assembly <p>Example merging paired-end data</p> sample fastq_1 fastq_2 my_sample path_to_forward_reads_1 path_to_reversed_reads_1 my_sample path_to_forward_reads_2 path_to_reversed_reads_2 <p>The value of <code>my_sample</code> is repeated twice allowing sample to be merged on the common key. This works for nanopore data, hybrid assembly data and assemblies</p>"},{"location":"usage/usage/#command-line-arguments","title":"Command line arguments","text":"<p>Note: All the below settings can be permanently changed in the <code>nextflow.config</code> file within the <code>params</code> section. For example, to permanently set a nanopore chemistry and use Kraken for speciation:</p> <pre><code>--run_kraken = true // Note the lack of quotes\n--nanopore_chemistry \"r1041_e82_400bps_hac_v4.2.0\" // Note the quotes used here\n</code></pre>"},{"location":"usage/usage/#nf-core-boiler-plate-options","title":"Nf-core boiler plate options","text":"<ul> <li><code>--publish_dir_mode</code>: Method used to save pipeline results to output directory</li> <li><code>--email</code>: Email address for completion summary.</li> <li><code>--email_on_fail</code>: An email address to send a summary email to when the pipeline is completed - ONLY sent if the pipeline does not exit successfully.</li> <li><code>--plaintext_email</code>: Send plain-text email instead of HTML.</li> <li><code>--monochrome_logs</code>: Do not use coloured log outputs.</li> <li><code>--hook_url</code>: Incoming hook URL for messaging service. Currently, MS Teams and Slack are supported.</li> <li><code>--help</code>: Display help text.</li> <li><code>--version</code>: Display version and exit.</li> <li><code>--validate_params</code>: Boolean whether to validate parameters against the schema at runtime.</li> <li><code>--show_hidden_params</code>: By default, parameters set as hidden in the schema are not shown on the command line when a user runs with <code>--help</code>. Specifying this option will tell the pipeline to show all parameters.</li> </ul>"},{"location":"usage/usage/#general-tool-options","title":"General tool options","text":"<ul> <li><code>--fly_read_type VALUE</code>: Flye allows for different assembly options. The default value is set too <code>hq</code> (High quality for Nanopore reads, and HiFi for bacterial reads). User options include <code>hq</code>, <code>corr</code> and <code>raw</code>, and a default value can be specified in the <code>nextflow.config</code> file.</li> <li><code>--hybrid_unicycler true</code>: to use unicycler for assembly instead of Flye-&gt;Racon&gt;Pilon.     &gt;Note: You may need to check the <code>conf/base.config</code> <code>process_high_memory</code> declaration and provide it upwards of 1000GB of memory if you get errors mentioning <code>tputs</code>. This error is not very clear sadly but increasing resources available to the process will help.</li> <li><code>--metagenomic_run true</code>: users can specify samples are metagenomic via this flag, the pipeline will skip running the contamination mash check and proceed with metagenomic assembly.</li> <li><code>--min_reads NUM</code>: refers to the minimum number of reads required after the fastP step to progress a set of sample reads to assembly (default: 1000).</li> <li><code>--nanopore_chemistry YOUR_MODEL_HERE</code>: a Medaka model must be specified for polishing. A list of allowed models can be found here: Medaka models python script or Medaka models available for download</li> <li><code>--run_kraken TRUE</code>: can be used to enable Kraken2 for speciation instead of Mash.</li> <li><code>--target_depth</code>: refers to the target bp depth for a set of reads. When sample read sets have an estimated depth higher than this target, it is downsampled to achieve the depth. No downsampling occurs when estimated depth is lower than this value (default 100).</li> </ul>"},{"location":"usage/usage/#skip-options","title":"Skip Options","text":"<p>Numerous steps within mikrokondo can be turned off without compromising the stability of the pipeline. This skip options can reduce run-time of the pipeline or allow for completion of the pipeline despite errors. ** All of the above options can be turned on by entering <code>--{skip_option} true</code> in the command line arguments to the pipeline (where optional parameters can be added)**</p> <ul> <li><code>--skip_read_merging</code>: Do not merge reads, if duplicate sample names are present the names will be made unique.</li> <li><code>--skip_abricate</code>: turn off abricate AMR detection</li> <li><code>--skip_bakta</code>: turn off bakta annotation pipeline (generally a slow step, requiring a database to be specified).</li> <li><code>--skip_checkm</code>: used as part of the contamination detection within mikrokondo, its run time and resource usage can be quite lengthy.</li> <li><code>--skip_depth_sampling</code>: genome size of reads is estimated using mash and reads can be down-sampled to target depth in order to have a better assembly, if this is of no interest to you, this flag will skip this step entirely. If you have specified that your run is metagenomic, down sampling is turned off.</li> <li><code>--skip_mobrecon</code>: turn off mob-suite recon.</li> <li><code>--skip_ont_header_cleaning</code>: Nanopore data may fail in the pipeline due to duplicate headers, while rare it can cause assemblies to fail. Unlike the other options on this list, skipping header cleaning is defaulted to TRUE.</li> <li><code>--skip_polishing</code>: if running a metagenomic assembly or encountering issues with polishing steps, this flag will disable polishing and retrieve assembly directly from Spades/Fly. this does not apply to hybrid assemblies.</li> <li><code>--skip_report</code>: prevents the generation of the final summary report.</li> <li><code>--skip_species_classification</code>: prevents Mash or Kraken2 being run on assembled genome, also prevents subtyping workflow from triggering.</li> <li><code>--skip_starmar</code>: turn off starAMR AMR detection.</li> <li><code>--skip_subtyping</code>: to turn off automatic triggering of subtyping in the pipeline (useful when target organism does not have a subtyping tool installed within mikrokondo).</li> <li><code>--skip_version_gathering</code>: prevents the collation of tool versions. This process generally takes a couple minutes (at worst) but can be useful when during recurrent runs of the pipeline (like when testing settings).</li> <li><code>--skip_report</code>: Prevents creation of final report summary report amalgamating outputs of all other files, this will also turnoff the creation of individual sub-reports.</li> <li><code>--skip_metagenomic_detection</code>: Skips classification of sample as metagenomic and forces a sample to be analyzed as an isolate.</li> <li><code>--skip_raw_read_metrics</code>: Prevents generation of raw read metrics, e.g. metrics generated about the reads before any trimming or filtering is performed.</li> <li><code>--skip_mlst</code>: Skip seven gene MLST.</li> <li><code>--skip_length_filtering_contigs</code>: Skip length filtering of contigs based on the <code>--qt_min_contig_length</code> parameter.</li> <li><code>--skip_allele_calling</code>: Skip allele calling with Locidex.</li> <li><code>--fail_on_metagenomic</code>: Samples that are found to be contaminated or metagenomic are not processed past auto detection saving users computational resources.</li> </ul>"},{"location":"usage/usage/#datasets","title":"Datasets","text":"<p>Different databases/pre-computed files are required for usage within mikrokondo. These can be downloaded or created by the user, and if not configured within the <code>nextflow.config</code> file they can be passed in as files with the following command-line arguments.</p> <ul> <li><code>--dehosting_idx</code>: The minimap2 index to be used for dehosting.</li> <li><code>--mash_sketch</code>: The mash sketch to be used for contamination detection and speciation.</li> <li><code>--bakta_db</code>: Bakta database for genome annotation.</li> <li><code>--kraken2_db</code>: Kraken2 database that can be used for speciation and binning of meta-genomically assembled contigs.</li> <li><code>--staramr_db</code>: An optional StarAMR database to be passed in, it is recommended to use the database packaged in the container.</li> </ul>"},{"location":"usage/usage/#allele-scheme-options","title":"Allele Scheme Options","text":"<p>Allele scheme selection parameters.</p> <ul> <li><code>--override_allele_scheme</code>: Provide the path to an allele scheme (currently only locidex is supported) that will be used for all samples provided. e.g. no automated allele database selection is performed, this scheme will be applied.</li> <li><code>--lx_allele_database</code>: A path to a <code>manifest.json</code> file used by locidex for automated allele selection. This option cannot be used along side <code>--override_allele_scheme</code>. <p>Note: The provide only a path to the <code>manifest.json</code> file as <code>some/directory</code> NOT <code>some/directory/manifest.json</code></p> </li> </ul>"},{"location":"usage/usage/#fastp-arguments","title":"FastP Arguments","text":"<p>For simplicity parameters affecting FastP have been moved to the top level. Each argument matches one listed within the FastP usage section with only a <code>fp_</code> being appended to the front of the argument. For a more detailed description of what each argument does please review the tool specific parameters for FastP here.</p> <ul> <li><code>--fp_average_quality</code>: If a read/read-pair quality is less than this value it is discarded</li> <li><code>--fp_cut_tail_mean_quality</code>: the quality threshold to trim reads to</li> <li><code>--fp_cut_tail_window_size</code>: The window size to cut a tail with.</li> <li><code>--fp_complexity_threshold</code>: The threshold for low complexity filter.</li> <li><code>--fp_qualified_phred</code>: The quality of a base to be qualified if filtering by unqualified bases.</li> <li><code>--fp_unqualified_percent_limit</code>: The percent amount of bases that are allowed to be unqualified in a read.</li> <li><code>--fp_polyg_min_len</code>: The minimum length to detect a polyG tail.</li> <li><code>--fp_polyx_min_len</code>: The minimum length to detect a polyX tail.</li> <li><code>--fp_illumina_length_min</code>: The minimum read length to be allowed in illumina data.</li> <li><code>--fp_illumina_length_max</code>: The maximum read length allowed for illumina data.</li> <li><code>--fp_single_end_length_min</code>: the minimum read length allowed in Pacbio or Nanopore data.</li> <li><code>--fp_dedup_reads</code>: A parameter to be turned on to allow for deduplication of reads.</li> </ul>"},{"location":"usage/usage/#bakta-parameters","title":"Bakta Parameters","text":"<p>Top level parameters that can be passed to Bakta.</p> <ul> <li><code>--ba_min_contig_length</code>: Minimum contig length to be analyzed by Bakta</li> </ul>"},{"location":"usage/usage/#quast-parameters","title":"Quast Parameters","text":"<p>Top level parameters that can be passed to Quast.</p> <ul> <li><code>--qt_min_contig_length</code>: Minimum length of a contig to be analyzed within Quast.</li> </ul>"},{"location":"usage/usage/#mash-parameters","title":"Mash Parameters","text":"<p>Top level parameters to be passed to Mash.</p> <ul> <li><code>--mh_min_kmer</code>: The minimum time a kmer needs to appear to be used in genome size estimation by mash.</li> </ul>"},{"location":"usage/usage/#ectyper-parameters","title":"ECTyper Parameters","text":"<p>Top level parameters to pass to ECTyper. Each argument corresponds to one within ECTyper.</p> <ul> <li><code>--ec_opid</code>: The minimum percent identity to determine an O antigens presence, It must be an integer.</li> <li><code>--ec_opcov</code>: The minimum percent coverage of O antigen, It must be an integer.</li> <li><code>--ec_hpid</code>: The minimum percent identity to determine an H antigens presence, It must be an integer.</li> <li><code>--ec_hcov</code>: The minimum percent coverage of the H antigen, It must be an integer.</li> <li><code>--ec_enable_verification</code>: A boolean value to enable species verification in ECTyper.</li> </ul>"},{"location":"usage/usage/#sistr-parameters","title":"SISTR Parameters","text":"<p>Top level parameters for SISTR.</p> <ul> <li><code>--sr_full_cgmlst</code>: A boolean value (default is true) to use the full cgMLST set of alleles for SISTR which includes some highly similar alleles.</li> </ul>"},{"location":"usage/usage/#locidex-parameters","title":"Locidex Parameters","text":"<p>Top level parameters for Locidex. The currently implemented allele caller, do not that internally Locidex uses blast so many of the parameters correspond to blast options.</p> <ul> <li><code>--lx_min_evalue</code>: Minimum e-value required for a match.</li> <li><code>--lx_min_dna_len</code>: Global minimum query length of DNA strand.</li> <li><code>--lx_min_aa_len</code>: Global minimum query length of an Amino Acid strand.</li> <li><code>--lx_max_dna_len</code>: Global maximum query length of DNA strand.</li> <li><code>--lx_max_aa_len</code>: Global maximum query length of Amino Acid strand.</li> <li><code>--lx_min_dna_ident</code>: Global minimum DNA percent identity required for match. (float).</li> <li><code>--lx_min_aa_ident</code>: Global minimum Amino Acid percent identity required for match. (float).</li> <li><code>--lx_min_dna_match_cov</code>: Global minimum DNA percent hit coverage identity required for match (float).</li> <li><code>--lx_min_aa_match_cov</code>: Global minimum Amino Acid hit coverage identity required for match (float).</li> <li><code>--lx_max_target_seqs</code>: Maximum number of sequence hits per query.</li> <li><code>--lx_extraction_mode</code>: Different ways to run locidex (Options: snps, trim, raw, extend).</li> <li><code>--lx_report_mode</code>: Allele profile assignment (Options: normal or conservative).</li> <li><code>--lx_report_prop</code>: Metadata label to use for aggregation. Only alphanumeric characters, underscores and dashes are allowed in names.</li> <li><code>--lx_report_max_ambig</code>: Maximum number of ambiguous characters allowed in a sequence.</li> <li><code>--lx_report_max_stop</code>: Maximum number of internal stop codons allowed in a sequence.</li> </ul>"},{"location":"usage/usage/#containers","title":"Containers","text":"<p>Different container services can be specified from the command line when running mikrokondo in the <code>-profile</code> option. This option is specified at the end of your command line argument. Examples of different container services are specified below:</p> <ul> <li>For Docker: <code>nextflow run main.nf MY_OPTIONS -profile docker</code></li> <li>For Singularity: <code>nextflow run main.nf MY_OPTIONS -profile singularity</code></li> <li>For Apptainer: <code>nextflow run main.nf MY_OPTIONS -profile apptainer</code></li> <li>For Shifter: <code>nextflow run main.nf MY_OPTIONS -profile shifter</code></li> <li>For Charliecloud: <code>nextflow run main.nf MY_OPTIONS -profile charliecloud</code></li> <li>For Gitpod: <code>nextflow run main.nf MY_OPTIONS -profile gitpod</code></li> <li>For Podman: <code>nextflow run main.nf MY_OPTIONS -profile podman</code></li> </ul>"},{"location":"usage/usage/#platform-specification","title":"Platform specification","text":"<ul> <li><code>--platform illumina</code> for Illumina.</li> <li><code>--platform nanopore</code> for Nanopore.</li> <li><code>--platform pacbio</code> for Pacbio</li> <li><code>--platform hybrid</code> for hybrid assemblies. <p>Note: when denoting your run as using a hybrid platform, you must also add in the long_read_opt parameter as the default value is nanopore**. <code>--long_read_opt nanopore</code> for nanopore or <code>--long_read_opt pacbio</code> for pacbio.</p> </li> </ul>"},{"location":"usage/usage/#slurm-options","title":"Slurm options","text":"<ul> <li><code>slurm_p true</code>: slurm executor will be used.</li> <li><code>slurm_profile STRING</code>: a string to allow the user to specify which slurm partition to use.</li> </ul>"},{"location":"usage/usage/#output","title":"Output","text":"<p>All output files will be written into the <code>outdir</code> (specified by the user). More explicit tool results can be found in both the Workflow and Subworkflow sections of the docs. Here is a brief description of the outdir structure:</p> <ul> <li>annotations - dir containing all annotation tool output.</li> <li>assembly - dir containing all assembly tool related output, including quality, 7 gene MLST and taxon determination.</li> <li>pipeline_info - dir containing all pipeline related information including software versions used and execution reports.</li> <li>ReadQuality - dir containing all read tool related output, including contamination, fastq, mash, and subsampled read sets (when present)</li> <li>subtyping - dir containing all subtyping tool related output, including SISTR, ECtyper, etc.</li> <li>SummaryReport - dir containing collated results files for all tools, including:</li> <li>Individual sample flattened json reports</li> <li>final_report - All tool results for all samples in both .json (including a flattened version) and .tsv format</li> <li>bco.json - data providence file generated from the nf-prov plug-in</li> <li>manifest.json - data providence file generated from the nf-prov plug-in</li> </ul>"},{"location":"workflows/CleanAssemble/","title":"Clean Assemble","text":""},{"location":"workflows/CleanAssemble/#workflowslocalcleanassemble","title":"workflows/local/CleanAssemble","text":""},{"location":"workflows/CleanAssemble/#included-sub-workflows","title":"Included sub-workflows","text":"<ul> <li><code>assemble_reads.nf</code></li> <li><code>clean_reads.nf</code></li> <li><code>hybrid_assembly.nf</code></li> <li><code>input_check.nf</code></li> <li><code>polish_assemblies.nf</code></li> </ul>"},{"location":"workflows/CleanAssemble/#steps","title":"Steps","text":"<ol> <li> <p>QC reads subworkflow steps in brief are listed below, for further information see clean_reads.nf</p> <ul> <li>Reads are checked for known sequencing contamination</li> <li>Quality metrics are calculated</li> <li>Reads are trimmed</li> <li>Coverage is estimated</li> <li>Read set subsampled to set level (OPTIONAL)</li> <li>Read set is assessed to be either an isolate or metagenomic sample (from presence of multiple taxa)</li> </ul> </li> <li> <p>Assemble reads using the <code>params.platform</code> flag, read sets will be diverted to either the assemble_reads (short reads) or hybrid_assembly (short and/or long reads) workflow. Though the data is handled differently in eash subworklow, both generate a contigs file and a bandage image, with an option of initial polishing via Racon. See assemble_reads.nf and hybrid_assembly.nf subworkflow pages for more details.</p> </li> <li> <p>Polish assembles (OPTIONAL) Polishing of contigs can be added polish_assemblies.nf. To make changes to the default workflow, see setting 'optional flags' page.</p> </li> </ol>"},{"location":"workflows/CleanAssemble/#input","title":"Input","text":"<ul> <li>Next generation sequencing reads:<ul> <li>Short read - Illumina</li> <li>Long read:<ul> <li>Nanopore</li> <li>Pacbio</li> </ul> </li> </ul> </li> </ul>"},{"location":"workflows/CleanAssemble/#output","title":"Output","text":"<ul> <li>Reads</li> <li>FinalReads<ul> <li>SAMPLE</li> </ul> </li> <li>Processing<ul> <li>Dehosting<ul> <li>Trimmed<ul> <li>FastP</li> <li>MashSketches</li> </ul> </li> </ul> </li> </ul> </li> <li>Quality<ul> <li>RawReadQuality</li> <li>Trimmed<ul> <li>FastP</li> <li>MashScreen</li> </ul> </li> </ul> </li> <li>Assembly<ul> <li>Assembling<ul> <li>Bandage</li> <li>ConsensusGeneration<ul> <li>Polishing<ul> <li>Pilon<ul> <li>BAMs</li> <li>Changes</li> <li>Fasta</li> <li>VCF</li> </ul> </li> </ul> </li> <li>Racon<ul> <li>Consensus</li> </ul> </li> </ul> </li> <li>Spades<ul> <li>Contigs</li> <li>GeneClusters</li> <li>Graphs</li> <li>Logs</li> <li>Scaffolds</li> <li>Transcripts</li> </ul> </li> </ul> </li> <li>FinalAssembly<ul> <li>SAMPLE</li> </ul> </li> </ul> </li> </ul> <p>NOTE: Bolded directories contain the nested structure of tool output. The further into the structure you go, the further along the workflow that that tool was run. </p>"},{"location":"workflows/PostAssembly/","title":"Post assembly","text":""},{"location":"workflows/PostAssembly/#workflowslocalpostassembly","title":"workflows/local/PostAssembly","text":"<p>This workflow is triggered in two ways: 1. when assemblies are used for initial input to the pipeline; and 2. after the <code>CleanAssemble.nf</code> workflow completes. Within this workflow, Quast, CheckM, species determination (Using Kraken2 or Mash), annotation and subtyping are all performed.</p>"},{"location":"workflows/PostAssembly/#included-sub-workflows","title":"Included sub-workflows","text":"<ul> <li><code>annotate_genomes.nf</code></li> <li><code>determine_species.nf</code></li> <li><code>polish_assemblies.nf</code></li> <li><code>qc_assemblies.nf</code></li> <li><code>split_metagenomic.nf</code></li> <li><code>subtype_genome.nf</code></li> </ul>"},{"location":"workflows/PostAssembly/#steps","title":"Steps","text":"<ol> <li>Determine type using the <code>metagenomic_samples</code> flag, this workflow will direct assemblies to the following two paths:<ol> <li>Isolate: proceeds to step 2.</li> <li>Metagenomic: runs the following two modules before proceeding to step 2.<ol> <li>kraken.nf runs kraken2 on contigs</li> <li>bin_kraken2.nf bins contigs to respective genus level taxa.</li> </ol> </li> </ol> </li> <li>QC assemblies (OPTIONAL) runs quast and assigns quality metrics to generated assemblies.</li> <li>Determine species (OPTIONAL) runs classifier tool (default: Mash) to determine sample or binned species.</li> <li>Subtype genome (OPTIONAL) species specific subtyping tools are launched using a generated MASH screen report.</li> <li>Annotate genome (OPTIONAL) tools for annotation and identification of genes of interest are launched as a part of this step.</li> </ol>"},{"location":"workflows/PostAssembly/#input","title":"Input","text":"<ul> <li>Contig file (fasta) from the <code>FinalAssembly</code> dir<ul> <li>This is the final contig file from the last step in the <code>CleanAssemble</code> workflow (taking into account any skip flags that have been used)</li> </ul> </li> </ul>"},{"location":"workflows/PostAssembly/#output","title":"Output","text":"<ul> <li>Subtyping<ul> <li>TYPINGTOOL<ul> <li>SAMPLE</li> </ul> </li> </ul> </li> <li>FinalReports<ul> <li>Aggregated<ul> <li>Json</li> <li>Tables</li> </ul> </li> <li>FlattenedReports</li> <li>Sample<ul> <li>Json</li> </ul> </li> </ul> </li> </ul> <p>SUBTYPING: Within the subtyping directory there will be directories for each of the different subtyping tools used during that run. The number and type of tools will differ depending on the organisms present in the set of samples submitted to the pipeline.</p> <p>FINAL REPORTS: Within mikrokondo, a number of reports have been created to collate the different tool outputs. These are a quicker way to view the final results for your sample runs.</p>"}]}